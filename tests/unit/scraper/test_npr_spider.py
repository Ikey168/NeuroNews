"""
Comprehensive tests for NPRSpider.
Tests NPR news article scraping, content extraction, and data validation.
"""

import pytest
from unittest.mock import MagicMock, patch
from scrapy.http import HtmlResponse, Request

# Import with proper path handling
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent / 'src'))

from scraper.spiders.npr_spider import NPRSpider
from scraper.items import NewsItem


class TestNPRSpider:
    """Test suite for NPRSpider class."""

    @pytest.fixture
    def spider(self):
        """NPRSpider fixture for testing."""
        return NPRSpider()

    @pytest.fixture
    def sample_main_page_html(self):
        """Sample NPR main page HTML for testing."""
        return """
        <html>
            <head><title>NPR</title></head>
            <body>
                <div class="homepage-container">
                    <a href="/2024/01/15/story1">Story 1 Title</a>
                    <a href="/2024/01/15/sections/tech/story2">Tech Story</a>
                    <a href="/2025/01/20/story3">Recent Story</a>
                    <a href="/sections/politics/2024/12/01/political-story">Political Story</a>
                    <a href="/old-story-2020">Old Story (should be filtered)</a>
                    <a href="https://external.com">External Link</a>
                </div>
            </body>
        </html>
        """

    @pytest.fixture
    def sample_article_html(self):
        """Sample NPR article HTML for testing."""
        return """
        <html>
            <head><title>Sample NPR Article</title></head>
            <body>
                <article>
                    <h1 class="storytitle">Sample Article Title</h1>
                    <div class="byline">
                        <span class="byline__name">
                            <a href="/author">John Reporter</a>
                        </span>
                    </div>
                    <time datetime="2024-01-15T10:00:00Z" class="date">
                        January 15, 2024
                    </time>
                    <div id="storytext">
                        <p>This is the first paragraph of the article.</p>
                        <p>This is the second paragraph with more details.</p>
                        <p>This is the third paragraph concluding the story.</p>
                    </div>
                    <div class="tags">
                        <a href="/tag/technology">Technology</a>
                        <a href="/tag/news">News</a>
                    </div>
                </article>
            </body>
        </html>
        """

    @pytest.fixture
    def alternative_article_html(self):
        """Alternative NPR article HTML structure for testing."""
        return """
        <html>
            <body>
                <h1 class="story-title">Alternative Title Format</h1>
                <div class="storytext">
                    <p>Content in alternative format.</p>
                    <p>More content here.</p>
                </div>
                <div class="byline">
                    <span>By Jane Reporter</span>
                </div>
                <div class="date">January 20, 2024</div>
            </body>
        </html>
        """

    def test_spider_initialization(self, spider):
        """Test NPRSpider initialization."""
        assert spider.name == "npr"
        assert "npr.org" in spider.allowed_domains
        assert "https://www.npr.org/" in spider.start_urls

    def test_parse_main_page(self, spider, sample_main_page_html):
        """Test parsing of NPR main page to extract article links."""
        url = "https://www.npr.org/"
        response = HtmlResponse(url=url, body=sample_main_page_html.encode('utf-8'))
        
        # Get all requests generated by the spider
        requests = list(spider.parse(response))
        
        # Should generate requests for articles with 2024/2025 in URL
        assert len(requests) >= 3
        
        # Check that proper URLs are being requested
        urls = [req.url for req in requests]
        assert any("story1" in url for url in urls)
        assert any("story2" in url for url in urls)
        assert any("story3" in url for url in urls)
        assert any("political-story" in url for url in urls)
        
        # Verify callback is set correctly
        for req in requests:
            assert req.callback == spider.parse_article

    def test_parse_article_standard_format(self, spider, sample_article_html):
        """Test parsing of NPR article with standard format."""
        url = "https://www.npr.org/2024/01/15/story1"
        response = HtmlResponse(url=url, body=sample_article_html.encode('utf-8'))
        
        # Parse the article
        items = list(spider.parse_article(response))
        
        assert len(items) == 1
        item = items[0]
        
        # Verify item structure
        assert isinstance(item, NewsItem)
        assert item['title'] == "Sample Article Title"
        assert item['url'] == url
        assert item['source'] == "NPR"
        assert item['author'] == "John Reporter"
        assert "2024" in item['published_date']
        
        # Verify content extraction
        content = item['content']
        assert "first paragraph" in content
        assert "second paragraph" in content
        assert "third paragraph" in content
        
        # Verify tags
        assert "Technology" in item['tags']
        assert "News" in item['tags']

    def test_parse_article_alternative_format(self, spider, alternative_article_html):
        """Test parsing of NPR article with alternative HTML structure."""
        url = "https://www.npr.org/2024/01/20/story2"
        response = HtmlResponse(url=url, body=alternative_article_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        
        assert len(items) == 1
        item = items[0]
        
        assert item['title'] == "Alternative Title Format"
        assert "alternative format" in item['content']
        assert item['author'] == "Jane Reporter"

    def test_parse_article_missing_elements(self, spider):
        """Test parsing article with missing elements."""
        minimal_html = """
        <html>
            <body>
                <h1>Just a Title</h1>
                <p>Minimal content</p>
            </body>
        </html>
        """
        
        url = "https://www.npr.org/2024/01/01/minimal"
        response = HtmlResponse(url=url, body=minimal_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        
        assert len(items) == 1
        item = items[0]
        
        assert item['title'] == "Just a Title"
        assert item['url'] == url
        assert item['source'] == "NPR"
        # Missing fields should have default values
        assert item['author'] == ""
        assert item['published_date'] == ""

    def test_parse_article_no_title(self, spider):
        """Test parsing article with no title."""
        no_title_html = """
        <html>
            <body>
                <div id="storytext">
                    <p>Content without title</p>
                </div>
            </body>
        </html>
        """
        
        url = "https://www.npr.org/2024/01/01/no-title"
        response = HtmlResponse(url=url, body=no_title_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        
        assert len(items) == 1
        item = items[0]
        
        assert item['title'] == ""
        assert "Content without title" in item['content']

    def test_content_extraction_priority(self, spider):
        """Test content extraction with multiple selector priorities."""
        priority_html = """
        <html>
            <body>
                <h1 class="storytitle">Priority Title</h1>
                <div id="storytext">
                    <p>Primary content location</p>
                </div>
                <div class="storytext">
                    <p>Secondary content location</p>
                </div>
                <div class="story-text">
                    <p>Tertiary content location</p>
                </div>
            </body>
        </html>
        """
        
        url = "https://www.npr.org/2024/01/01/priority-test"
        response = HtmlResponse(url=url, body=priority_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        item = items[0]
        
        # Should use primary content selector first
        assert "Primary content location" in item['content']
        assert "Secondary content location" not in item['content']

    def test_author_extraction_variations(self, spider):
        """Test author extraction with different HTML structures."""
        author_variants_html = """
        <html>
            <body>
                <h1>Test Article</h1>
                <div class="byline">
                    <span class="byline__name">
                        <a href="/author">Linked Author</a>
                    </span>
                </div>
                <div id="storytext">
                    <p>Article content</p>
                </div>
            </body>
        </html>
        """
        
        url = "https://www.npr.org/2024/01/01/author-test"
        response = HtmlResponse(url=url, body=author_variants_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        item = items[0]
        
        assert item['author'] == "Linked Author"

    def test_date_extraction_formats(self, spider):
        """Test date extraction with various formats."""
        date_format_html = """
        <html>
            <body>
                <h1>Date Test Article</h1>
                <time datetime="2024-01-15T10:00:00Z" class="date">
                    January 15, 2024
                </time>
                <div id="storytext">
                    <p>Content with date</p>
                </div>
            </body>
        </html>
        """
        
        url = "https://www.npr.org/2024/01/15/date-test"
        response = HtmlResponse(url=url, body=date_format_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        item = items[0]
        
        # Should extract either datetime attribute or text
        assert "2024" in item['published_date']

    def test_tag_extraction(self, spider):
        """Test tag extraction from articles."""
        tagged_html = """
        <html>
            <body>
                <h1>Tagged Article</h1>
                <div class="tags">
                    <a href="/tag/politics">Politics</a>
                    <a href="/tag/economy">Economy</a>
                    <a href="/tag/analysis">Analysis</a>
                </div>
                <div id="storytext">
                    <p>Tagged content</p>
                </div>
            </body>
        </html>
        """
        
        url = "https://www.npr.org/2024/01/01/tagged-article"
        response = HtmlResponse(url=url, body=tagged_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        item = items[0]
        
        tags = item['tags']
        assert "Politics" in tags
        assert "Economy" in tags
        assert "Analysis" in tags

    def test_url_filtering_in_parse(self, spider):
        """Test URL filtering logic in main parse method."""
        mixed_urls_html = """
        <html>
            <body>
                <a href="/2024/01/15/valid-story">Valid Story</a>
                <a href="/2023/12/31/old-story">Old Story</a>
                <a href="/sections/tech/2024/01/20/tech-story">Tech Story</a>
                <a href="/about/contact">Contact Page</a>
                <a href="https://external.com/article">External Article</a>
                <a href="/story/without-date">No Date Story</a>
            </body>
        </html>
        """
        
        url = "https://www.npr.org/"
        response = HtmlResponse(url=url, body=mixed_urls_html.encode('utf-8'))
        
        requests = list(spider.parse(response))
        urls = [req.url for req in requests]
        
        # Should include 2024/2025 stories and stories with /sections/ or /story/
        assert any("valid-story" in url for url in urls)
        assert any("tech-story" in url for url in urls)
        
        # Should filter out non-story URLs
        assert not any("contact" in url for url in urls)
        assert not any("external.com" in url for url in urls)

    def test_content_cleaning(self, spider):
        """Test content cleaning and processing."""
        messy_html = """
        <html>
            <body>
                <h1 class="storytitle">Clean Title</h1>
                <div id="storytext">
                    <p>   First paragraph with extra spaces   </p>
                    <p>
                        Second paragraph
                        with line breaks
                    </p>
                    <p><strong>Bold text</strong> and <em>italic text</em></p>
                    <script>malicious_script()</script>
                </div>
            </body>
        </html>
        """
        
        url = "https://www.npr.org/2024/01/01/clean-test"
        response = HtmlResponse(url=url, body=messy_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        item = items[0]
        
        content = item['content']
        
        # Should clean up extra spaces and line breaks
        assert "First paragraph with extra spaces" in content
        assert "Second paragraph with line breaks" in content
        assert "Bold text and italic text" in content
        
        # Should not contain script tags
        assert "malicious_script" not in content

    def test_empty_response_handling(self, spider):
        """Test handling of empty or invalid responses."""
        empty_html = "<html><body></body></html>"
        
        url = "https://www.npr.org/2024/01/01/empty"
        response = HtmlResponse(url=url, body=empty_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        
        assert len(items) == 1
        item = items[0]
        
        # Should still create an item with basic info
        assert item['url'] == url
        assert item['source'] == "NPR"
        assert item['title'] == ""
        assert item['content'] == ""

    def test_relative_url_handling(self, spider, sample_main_page_html):
        """Test handling of relative URLs in link extraction."""
        url = "https://www.npr.org/"
        response = HtmlResponse(url=url, body=sample_main_page_html.encode('utf-8'))
        
        requests = list(spider.parse(response))
        
        # All URLs should be absolute
        for req in requests:
            assert req.url.startswith("https://")
            assert "npr.org" in req.url

    def test_error_handling_in_parse_article(self, spider):
        """Test error handling in article parsing."""
        malformed_html = """
        <html>
            <body>
                <h1 class="storytitle">Test Title
                <div id="storytext">
                    <p>Unclosed paragraph
                    <p>Another paragraph</p>
            </body>
        """
        
        url = "https://www.npr.org/2024/01/01/malformed"
        response = HtmlResponse(url=url, body=malformed_html.encode('utf-8'))
        
        # Should not raise exceptions
        items = list(spider.parse_article(response))
        
        assert len(items) == 1
        item = items[0]
        assert item['url'] == url

    def test_large_content_handling(self, spider):
        """Test handling of articles with large content."""
        # Create very large content
        large_content = "<p>" + "Very long content. " * 1000 + "</p>"
        large_html = f"""
        <html>
            <body>
                <h1 class="storytitle">Large Article</h1>
                <div id="storytext">
                    {large_content}
                </div>
            </body>
        </html>
        """
        
        url = "https://www.npr.org/2024/01/01/large-article"
        response = HtmlResponse(url=url, body=large_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        item = items[0]
        
        assert len(item['content']) > 10000
        assert "Very long content" in item['content']

    def test_unicode_content_handling(self, spider):
        """Test handling of unicode characters in content."""
        unicode_html = """
        <html>
            <body>
                <h1 class="storytitle">Unicode Test: Café & Résumé</h1>
                <div id="storytext">
                    <p>Content with émojis 🌟 and special chars: ñáéíóú</p>
                    <p>Chinese characters: 你好世界</p>
                    <p>Arabic text: مرحبا بالعالم</p>
                </div>
            </body>
        </html>
        """
        
        url = "https://www.npr.org/2024/01/01/unicode-test"
        response = HtmlResponse(url=url, body=unicode_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        item = items[0]
        
        assert "Café & Résumé" in item['title']
        assert "émojis 🌟" in item['content']
        assert "你好世界" in item['content']
        assert "مرحبا بالعالم" in item['content']

    def test_metadata_extraction(self, spider):
        """Test extraction of article metadata."""
        metadata_html = """
        <html>
            <head>
                <meta name="description" content="Article description">
                <meta name="keywords" content="news, politics, economy">
                <meta property="article:published_time" content="2024-01-15T10:00:00Z">
                <meta property="article:author" content="Meta Author">
            </head>
            <body>
                <h1 class="storytitle">Metadata Test</h1>
                <div id="storytext">
                    <p>Article with metadata</p>
                </div>
            </body>
        </html>
        """
        
        url = "https://www.npr.org/2024/01/15/metadata-test"
        response = HtmlResponse(url=url, body=metadata_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        item = items[0]
        
        # Basic extraction should work even with metadata present
        assert item['title'] == "Metadata Test"
        assert "Article with metadata" in item['content']

    def test_spider_custom_settings(self, spider):
        """Test spider custom settings and configurations."""
        # NPR spider might have custom settings for rate limiting, etc.
        settings = getattr(spider, 'custom_settings', {})
        
        # Verify reasonable defaults exist or can be set
        assert spider.name == "npr"
        assert len(spider.allowed_domains) > 0

    def test_request_meta_data(self, spider, sample_main_page_html):
        """Test that requests include proper meta data."""
        url = "https://www.npr.org/"
        response = HtmlResponse(url=url, body=sample_main_page_html.encode('utf-8'))
        
        requests = list(spider.parse(response))
        
        for req in requests:
            # Requests should have proper callback
            assert req.callback == spider.parse_article
            # Meta data can be added if needed for tracking
            assert hasattr(req, 'meta')