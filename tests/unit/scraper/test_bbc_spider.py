"""
Comprehensive tests for BBCSpider.
Tests BBC news article scraping, content extraction, and data validation.
"""

import pytest
import re
from unittest.mock import MagicMock, patch
from scrapy.http import HtmlResponse, Request

# Import with proper path handling
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent / 'src'))

from scraper.spiders.bbc_spider import BBCSpider
from scraper.items import NewsItem


class TestBBCSpider:
    """Test suite for BBCSpider class."""

    @pytest.fixture
    def spider(self):
        """BBCSpider fixture for testing."""
        return BBCSpider()

    @pytest.fixture
    def sample_main_page_html(self):
        """Sample BBC main page HTML for testing."""
        return """
        <html>
            <head><title>BBC News</title></head>
            <body>
                <div class="media-list">
                    <a href="/news/world-12345678">World News Story</a>
                    <a href="/news/uk-politics-87654321">UK Politics Story</a>
                    <a href="/news/business-11223344">Business Story</a>
                    <a href="/sport/football">Football (should be filtered)</a>
                    <a href="/news/invalid-link">Invalid News Link</a>
                    <a href="https://external.com">External Link</a>
                </div>
            </body>
        </html>
        """

    @pytest.fixture
    def sample_article_html(self):
        """Sample BBC article HTML for testing."""
        return """
        <html>
            <head><title>BBC News Article</title></head>
            <body>
                <article>
                    <h1 data-testid="headline">Sample BBC Article Title</h1>
                    <div class="byline">
                        <span>By BBC Correspondent</span>
                    </div>
                    <time datetime="2024-01-15T14:30:00Z" class="date">
                        15 January 2024
                    </time>
                    <div data-component="text-block">
                        <p>This is the first paragraph of the BBC article.</p>
                    </div>
                    <div data-component="text-block">
                        <p>This is the second paragraph with more details.</p>
                    </div>
                    <div data-component="text-block">
                        <p>This is the third paragraph concluding the story.</p>
                    </div>
                </article>
            </body>
        </html>
        """

    @pytest.fixture
    def legacy_article_html(self):
        """Legacy BBC article HTML structure for testing."""
        return """
        <html>
            <body>
                <h1 class="story-headline">Legacy BBC Article Title</h1>
                <div class="story-body__inner">
                    <p>Content in legacy format.</p>
                    <p>More legacy content here.</p>
                </div>
                <div class="byline">
                    <span class="byline__name">Legacy Reporter</span>
                </div>
                <div class="date">15 January 2024</div>
            </body>
        </html>
        """

    def test_spider_initialization(self, spider):
        """Test BBCSpider initialization."""
        assert spider.name == "bbc"
        assert "bbc.com" in spider.allowed_domains
        assert "https://www.bbc.com/news" in spider.start_urls

    def test_parse_main_page(self, spider, sample_main_page_html):
        """Test parsing of BBC main page to extract article links."""
        url = "https://www.bbc.com/news"
        response = HtmlResponse(url=url, body=sample_main_page_html.encode('utf-8'))
        
        # Get all requests generated by the spider
        requests = list(spider.parse(response))
        
        # Should generate requests for articles with proper BBC news URLs
        assert len(requests) >= 3
        
        # Check that proper URLs are being requested
        urls = [req.url for req in requests]
        assert any("world-12345678" in url for url in urls)
        assert any("uk-politics-87654321" in url for url in urls)
        assert any("business-11223344" in url for url in urls)
        
        # Should filter out non-news URLs
        assert not any("football" in url for url in urls)
        
        # Verify callback is set correctly
        for req in requests:
            assert req.callback == spider.parse_article

    def test_url_filtering_regex(self, spider):
        """Test URL filtering with regex pattern."""
        test_urls = [
            "/news/world-12345678",  # Valid
            "/news/uk-politics-87654321",  # Valid
            "/news/business-11223344",  # Valid
            "/news/technology-99887766",  # Valid
            "/news/invalid-link",  # Invalid (no numbers)
            "/sport/football-12345678",  # Invalid (not news)
            "/weather",  # Invalid
        ]
        
        valid_urls = []
        for url in test_urls:
            if re.search(r"/news/[a-z-]+-\d+", url):
                valid_urls.append(url)
        
        assert len(valid_urls) == 4
        assert "/news/world-12345678" in valid_urls
        assert "/news/uk-politics-87654321" in valid_urls
        assert "/news/business-11223344" in valid_urls
        assert "/news/technology-99887766" in valid_urls

    def test_parse_article_modern_format(self, spider, sample_article_html):
        """Test parsing of BBC article with modern format."""
        url = "https://www.bbc.com/news/world-12345678"
        response = HtmlResponse(url=url, body=sample_article_html.encode('utf-8'))
        
        # Parse the article
        items = list(spider.parse_article(response))
        
        assert len(items) == 1
        item = items[0]
        
        # Verify item structure
        assert isinstance(item, NewsItem)
        assert item['title'] == "Sample BBC Article Title"
        assert item['url'] == url
        assert item['source'] == "BBC"
        assert item['author'] == "BBC Correspondent"
        assert "2024" in item['published_date']
        
        # Verify content extraction
        content = item['content']
        assert "first paragraph of the BBC article" in content
        assert "second paragraph with more details" in content
        assert "third paragraph concluding" in content

    def test_parse_article_legacy_format(self, spider, legacy_article_html):
        """Test parsing of BBC article with legacy HTML structure."""
        url = "https://www.bbc.com/news/legacy-87654321"
        response = HtmlResponse(url=url, body=legacy_article_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        
        assert len(items) == 1
        item = items[0]
        
        assert item['title'] == "Legacy BBC Article Title"
        assert "legacy format" in item['content']
        assert item['author'] == "Legacy Reporter"

    def test_parse_article_minimal_content(self, spider):
        """Test parsing article with minimal content."""
        minimal_html = """
        <html>
            <body>
                <h1>Just a BBC Title</h1>
                <div data-component="text-block">
                    <p>Minimal BBC content</p>
                </div>
            </body>
        </html>
        """
        
        url = "https://www.bbc.com/news/minimal-12345678"
        response = HtmlResponse(url=url, body=minimal_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        
        assert len(items) == 1
        item = items[0]
        
        assert item['title'] == "Just a BBC Title"
        assert item['url'] == url
        assert item['source'] == "BBC"
        assert "Minimal BBC content" in item['content']

    def test_title_extraction_fallback(self, spider):
        """Test title extraction with multiple selectors."""
        fallback_html = """
        <html>
            <body>
                <h1>Fallback Title</h1>
                <div data-component="text-block">
                    <p>Content with fallback title</p>
                </div>
            </body>
        </html>
        """
        
        url = "https://www.bbc.com/news/fallback-12345678"
        response = HtmlResponse(url=url, body=fallback_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        item = items[0]
        
        # Should use the fallback h1::text selector
        assert item['title'] == "Fallback Title"

    def test_content_extraction_priority(self, spider):
        """Test content extraction selector priority."""
        priority_html = """
        <html>
            <body>
                <h1 data-testid="headline">Priority Test Article</h1>
                <div data-component="text-block">
                    <p>Primary content selector</p>
                </div>
                <div class="story-body__inner">
                    <p>Secondary content selector</p>
                </div>
                <div class="story-text">
                    <p>Tertiary content selector</p>
                </div>
            </body>
        </html>
        """
        
        url = "https://www.bbc.com/news/priority-12345678"
        response = HtmlResponse(url=url, body=priority_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        item = items[0]
        
        # Should use primary content selector
        assert "Primary content selector" in item['content']
        assert "Secondary content selector" not in item['content']

    def test_author_extraction_variations(self, spider):
        """Test author extraction with different patterns."""
        author_variants = [
            ('By BBC Reporter', 'BBC Reporter'),
            ('BBC Political Correspondent', 'BBC Political Correspondent'),
            ('By John Smith, BBC News', 'John Smith, BBC News'),
            ('BBC', 'BBC'),
        ]
        
        for author_text, expected in author_variants:
            html = f"""
            <html>
                <body>
                    <h1>Test Article</h1>
                    <div class="byline">
                        <span>{author_text}</span>
                    </div>
                    <div data-component="text-block">
                        <p>Test content</p>
                    </div>
                </body>
            </html>
            """
            
            url = "https://www.bbc.com/news/author-test-12345678"
            response = HtmlResponse(url=url, body=html.encode('utf-8'))
            
            items = list(spider.parse_article(response))
            item = items[0]
            
            assert expected in item['author']

    def test_date_extraction(self, spider):
        """Test date extraction from BBC articles."""
        date_html = """
        <html>
            <body>
                <h1>Date Test Article</h1>
                <time datetime="2024-01-15T14:30:00Z" class="date">
                    15 January 2024
                </time>
                <div data-component="text-block">
                    <p>Content with date</p>
                </div>
            </body>
        </html>
        """
        
        url = "https://www.bbc.com/news/date-test-12345678"
        response = HtmlResponse(url=url, body=date_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        item = items[0]
        
        # Should extract date information
        assert "15 January 2024" in item['published_date']

    def test_relative_url_conversion(self, spider, sample_main_page_html):
        """Test conversion of relative URLs to absolute URLs."""
        url = "https://www.bbc.com/news"
        response = HtmlResponse(url=url, body=sample_main_page_html.encode('utf-8'))
        
        requests = list(spider.parse(response))
        
        # All URLs should be absolute
        for req in requests:
            assert req.url.startswith("https://")
            assert "bbc.com" in req.url

    def test_empty_article_handling(self, spider):
        """Test handling of articles with no content."""
        empty_html = """
        <html>
            <body>
                <h1 data-testid="headline">Empty Article</h1>
                <!-- No content blocks -->
            </body>
        </html>
        """
        
        url = "https://www.bbc.com/news/empty-12345678"
        response = HtmlResponse(url=url, body=empty_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        
        assert len(items) == 1
        item = items[0]
        
        assert item['title'] == "Empty Article"
        assert item['content'] == ""
        assert item['url'] == url

    def test_content_cleaning(self, spider):
        """Test content cleaning and text processing."""
        messy_html = """
        <html>
            <body>
                <h1 data-testid="headline">Messy Content Test</h1>
                <div data-component="text-block">
                    <p>   Paragraph with extra spaces   </p>
                </div>
                <div data-component="text-block">
                    <p>
                        Paragraph
                        with line breaks
                    </p>
                </div>
                <div data-component="text-block">
                    <p><strong>Bold</strong> and <em>italic</em> text</p>
                </div>
            </body>
        </html>
        """
        
        url = "https://www.bbc.com/news/messy-12345678"
        response = HtmlResponse(url=url, body=messy_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        item = items[0]
        
        content = item['content']
        
        # Should clean up spacing and preserve text
        assert "Paragraph with extra spaces" in content
        assert "Paragraph with line breaks" in content
        assert "Bold and italic text" in content

    def test_multiple_content_blocks(self, spider):
        """Test handling of articles with many content blocks."""
        multi_block_html = """
        <html>
            <body>
                <h1 data-testid="headline">Multi-block Article</h1>
                <div data-component="text-block"><p>Block 1</p></div>
                <div data-component="text-block"><p>Block 2</p></div>
                <div data-component="text-block"><p>Block 3</p></div>
                <div data-component="text-block"><p>Block 4</p></div>
                <div data-component="text-block"><p>Block 5</p></div>
            </body>
        </html>
        """
        
        url = "https://www.bbc.com/news/multi-12345678"
        response = HtmlResponse(url=url, body=multi_block_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        item = items[0]
        
        content = item['content']
        
        # Should include all blocks
        for i in range(1, 6):
            assert f"Block {i}" in content

    def test_special_characters_handling(self, spider):
        """Test handling of special characters in content."""
        special_chars_html = """
        <html>
            <body>
                <h1 data-testid="headline">Special Characters: £$€¥ & "quotes"</h1>
                <div data-component="text-block">
                    <p>Content with £100, €50, $25, ¥1000 and "quoted text"</p>
                </div>
                <div data-component="text-block">
                    <p>More symbols: © ® ™ & ampersands</p>
                </div>
            </body>
        </html>
        """
        
        url = "https://www.bbc.com/news/special-12345678"
        response = HtmlResponse(url=url, body=special_chars_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        item = items[0]
        
        # Should preserve special characters
        assert "£$€¥" in item['title']
        assert "£100" in item['content']
        assert "quoted text" in item['content']
        assert "© ® ™" in item['content']

    def test_error_handling_malformed_html(self, spider):
        """Test error handling with malformed HTML."""
        malformed_html = """
        <html>
            <body>
                <h1 data-testid="headline">Malformed Test
                <div data-component="text-block">
                    <p>Unclosed paragraph
                    <p>Another paragraph</p>
            </body>
        """
        
        url = "https://www.bbc.com/news/malformed-12345678"
        response = HtmlResponse(url=url, body=malformed_html.encode('utf-8'))
        
        # Should not raise exceptions
        items = list(spider.parse_article(response))
        
        assert len(items) == 1
        item = items[0]
        assert item['url'] == url
        assert "BBC" == item['source']

    def test_large_article_handling(self, spider):
        """Test handling of very large articles."""
        # Create article with many paragraphs
        content_blocks = []
        for i in range(50):
            content_blocks.append(f'<div data-component="text-block"><p>Paragraph {i+1} with substantial content.</p></div>')
        
        large_html = f"""
        <html>
            <body>
                <h1 data-testid="headline">Large BBC Article</h1>
                {''.join(content_blocks)}
            </body>
        </html>
        """
        
        url = "https://www.bbc.com/news/large-12345678"
        response = HtmlResponse(url=url, body=large_html.encode('utf-8'))
        
        items = list(spider.parse_article(response))
        item = items[0]
        
        # Should handle large content
        assert len(item['content']) > 1000
        assert "Paragraph 1" in item['content']
        assert "Paragraph 50" in item['content']