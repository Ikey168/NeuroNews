#!/usr/bin/env python3
"""
Definition of Done (DoD) verification for Issue #232
Hybrid retrieval + (optional) cross-encoder rerank

DoD Requirements:
• retriever.search(query) returns [{chunk, score, source, url}] with length ≤ K
• Candidate fetch: top-k from vector + top-k from lexical → union
• Score fusion: weighted sum or Reciprocal Rank Fusion
• Optional reranker (e.g., cross-encoder/ms-marco-MiniLM-L-6-v2); gated by env
"""

import sys
import os
import numpy as np
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from services.rag import (
    HybridRetriever, HybridSearchFilters,
    VectorSearchService, LexicalSearchService, CrossEncoderReranker,
    get_hybrid_retriever, hybrid_search
)


def simulate_embedding(text: str, dim: int = 384) -> np.ndarray:
    """Simulate text embedding using hash-based approach."""
    import hashlib
    
    hash_obj = hashlib.md5(text.encode())
    seed = int(hash_obj.hexdigest()[:8], 16)
    np.random.seed(seed)
    
    embedding = np.random.normal(0, 1, dim)
    embedding = embedding / np.linalg.norm(embedding)
    
    return embedding


def test_retriever_search_api():
    """
    DoD: retriever.search(query) returns [{chunk, score, source, url}] with length ≤ K
    """
    print("1️⃣  Testing retriever.search() API compliance")
    print("-" * 50)
    
    try:
        with get_hybrid_retriever() as retriever:
            query = "test query for API compliance"
            query_embedding = simulate_embedding(query)
            k = 5
            
            # Test the search method exists and returns correct format
            results = retriever.search(
                query=query,
                query_embedding=query_embedding,
                k=k
            )
            
            # Verify return type and length
            assert isinstance(results, list), f"Expected list, got {type(results)}"
            assert len(results) <= k, f"Expected ≤{k} results, got {len(results)}"
            
            # Verify result structure
            if results:
                result = results[0]
                required_attrs = ['content', 'final_score', 'source', 'url']
                
                for attr in required_attrs:
                    assert hasattr(result, attr), f"Result missing attribute: {attr}"
                
                # Test convenience function format
                dict_results = hybrid_search(query, query_embedding, k=3)
                assert isinstance(dict_results, list), "Convenience function should return list"
                
                if dict_results:
                    dict_result = dict_results[0]
                    required_keys = ['chunk', 'score', 'source', 'url']
                    
                    for key in required_keys:
                        assert key in dict_result, f"Dict result missing key: {key}"
            
            print(f"✅ retriever.search() returns proper format with length {len(results)} ≤ {k}")
            print(f"✅ Results have required fields: chunk, score, source, url")
            return True
            
    except Exception as e:
        print(f"❌ API compliance test failed: {e}")
        return False


def test_candidate_fetch_union():
    """
    DoD: Candidate fetch: top-k from vector + top-k from lexical → union
    """
    print("\n2️⃣  Testing candidate fetch and union")
    print("-" * 50)
    
    try:
        with get_hybrid_retriever() as retriever:
            query = "machine learning artificial intelligence"
            query_embedding = simulate_embedding(query)
            
            # Test that both vector and lexical services are used
            results = retriever.search(
                query=query,
                query_embedding=query_embedding,
                k=10,
                vector_k=5,
                lexical_k=5
            )
            
            # Check for results from different search methods
            vector_results = [r for r in results if r.search_method in ['vector', 'both']]
            lexical_results = [r for r in results if r.search_method in ['lexical', 'both']]
            both_results = [r for r in results if r.search_method == 'both']
            
            print(f"✅ Found {len(vector_results)} results with vector component")
            print(f"✅ Found {len(lexical_results)} results with lexical component")
            print(f"✅ Found {len(both_results)} results from both methods (union)")
            
            # Verify score components exist
            has_vector_scores = any(r.vector_score is not None for r in results)
            has_lexical_scores = any(r.lexical_score is not None for r in results)
            
            print(f"✅ Vector scores present: {has_vector_scores}")
            print(f"✅ Lexical scores present: {has_lexical_scores}")
            
            return True
            
    except Exception as e:
        print(f"❌ Candidate fetch test failed: {e}")
        return False


def test_score_fusion_methods():
    """
    DoD: Score fusion: weighted sum or Reciprocal Rank Fusion
    """
    print("\n3️⃣  Testing score fusion methods")
    print("-" * 50)
    
    try:
        with get_hybrid_retriever() as retriever:
            query = "neural networks deep learning"
            query_embedding = simulate_embedding(query)
            
            # Test weighted sum fusion
            weighted_results = retriever.search(
                query=query,
                query_embedding=query_embedding,
                k=5,
                fusion_method="weighted"
            )
            
            # Test RRF fusion
            rrf_results = retriever.search(
                query=query,
                query_embedding=query_embedding,
                k=5,
                fusion_method="rrf"
            )
            
            # Test max fusion
            max_results = retriever.search(
                query=query,
                query_embedding=query_embedding,
                k=5,
                fusion_method="max"
            )
            
            # Verify fusion scores are computed
            for method, results in [("weighted", weighted_results), ("rrf", rrf_results), ("max", max_results)]:
                if results:
                    result = results[0]
                    assert hasattr(result, 'fusion_score'), f"Missing fusion_score in {method}"
                    assert result.fusion_score >= 0, f"Invalid fusion_score in {method}: {result.fusion_score}"
                    print(f"✅ {method.upper()} fusion working: score = {result.fusion_score:.4f}")
                else:
                    print(f"⚠️  {method.upper()} fusion returned no results")
            
            return True
            
    except Exception as e:
        print(f"❌ Score fusion test failed: {e}")
        return False


def test_optional_reranker():
    """
    DoD: Optional reranker (e.g., cross-encoder/ms-marco-MiniLM-L-6-v2); gated by env
    """
    print("\n4️⃣  Testing optional cross-encoder reranker")
    print("-" * 50)
    
    try:
        # Test reranker creation and configuration
        reranker = CrossEncoderReranker(model_name="cross-encoder/ms-marco-MiniLM-L-6-v2")
        
        model_info = reranker.get_model_info()
        print(f"✅ Reranker model: {model_info['model_name']}")
        print(f"✅ Environment gated: {model_info['is_enabled']}")
        print(f"✅ Model loaded: {model_info['model_loaded']}")
        print(f"✅ Has sentence-transformers: {model_info['has_sentence_transformers']}")
        
        # Test environment variable gating
        original_env = os.environ.get('ENABLE_RERANKING', '')
        
        # Test disabled
        os.environ['ENABLE_RERANKING'] = 'false'
        disabled_reranker = CrossEncoderReranker()
        assert not disabled_reranker.is_enabled, "Reranker should be disabled when env var is false"
        print(f"✅ Reranker properly disabled by environment variable")
        
        # Test enabled 
        os.environ['ENABLE_RERANKING'] = 'true'
        enabled_reranker = CrossEncoderReranker()
        assert enabled_reranker.is_enabled, "Reranker should be enabled when env var is true"
        print(f"✅ Reranker properly enabled by environment variable")
        
        # Restore original environment
        if original_env:
            os.environ['ENABLE_RERANKING'] = original_env
        elif 'ENABLE_RERANKING' in os.environ:
            del os.environ['ENABLE_RERANKING']
        
        # Test reranking in retriever
        with get_hybrid_retriever(enable_reranking=True) as retriever:
            query = "quantum computing applications"
            query_embedding = simulate_embedding(query)
            
            # Test with reranking enabled
            results_with_rerank = retriever.search(
                query=query,
                query_embedding=query_embedding,
                k=3,
                enable_reranking=True
            )
            
            # Test with reranking disabled
            results_no_rerank = retriever.search(
                query=query,
                query_embedding=query_embedding,
                k=3,
                enable_reranking=False
            )
            
            print(f"✅ Search with reranking: {len(results_with_rerank)} results")
            print(f"✅ Search without reranking: {len(results_no_rerank)} results")
            
            # Verify reranking affects scores (if reranker is available)
            if results_with_rerank and results_no_rerank:
                rerank_score = results_with_rerank[0].final_score
                no_rerank_score = results_no_rerank[0].final_score
                print(f"✅ Reranking score difference detected: {abs(rerank_score - no_rerank_score):.4f}")
        
        return True
        
    except Exception as e:
        print(f"❌ Reranker test failed: {e}")
        return False


def test_files_exist():
    """
    DoD: Files exist as specified in issue
    """
    print("\n5️⃣  Testing required files exist")
    print("-" * 50)
    
    required_files = [
        "services/rag/retriever.py",
        "services/rag/rerank.py"
    ]
    
    all_exist = True
    for file_path in required_files:
        full_path = project_root / file_path
        if full_path.exists():
            print(f"✅ {file_path} exists")
        else:
            print(f"❌ {file_path} missing")
            all_exist = False
    
    # Check if services are properly exported
    try:
        from services.rag import HybridRetriever, CrossEncoderReranker
        print(f"✅ Services properly exported from services.rag")
    except ImportError as e:
        print(f"❌ Import error: {e}")
        all_exist = False
    
    return all_exist


def main():
    """Run all DoD verification tests."""
    print("🔍 Issue #232 DoD Verification")
    print("=" * 60)
    print("Hybrid retrieval + (optional) cross-encoder rerank")
    print("=" * 60)
    
    test_functions = [
        test_files_exist,
        test_retriever_search_api,
        test_candidate_fetch_union,
        test_score_fusion_methods,
        test_optional_reranker,
    ]
    
    results = []
    for test_func in test_functions:
        try:
            result = test_func()
            results.append(result)
        except Exception as e:
            print(f"❌ Test {test_func.__name__} failed with exception: {e}")
            results.append(False)
    
    print("\n" + "=" * 60)
    print("📋 DoD VERIFICATION SUMMARY")
    print("=" * 60)
    
    success_count = sum(results)
    total_count = len(results)
    
    test_names = [
        "Required files exist",
        "retriever.search() API compliance",
        "Candidate fetch and union",
        "Score fusion methods",
        "Optional reranker"
    ]
    
    for i, (name, result) in enumerate(zip(test_names, results)):
        status = "✅ PASS" if result else "❌ FAIL"
        print(f"{i+1}. {name}: {status}")
    
    print(f"\n🎯 Overall: {success_count}/{total_count} DoD requirements verified")
    
    if success_count == total_count:
        print("\n🎉 All DoD requirements satisfied!")
        print("\n📋 Issue #232 Requirements Summary:")
        print("✅ Candidate fetch: top-k from vector + top-k from lexical → union")
        print("✅ Score fusion: weighted sum or Reciprocal Rank Fusion")
        print("✅ Optional reranker (cross-encoder/ms-marco-MiniLM-L-6-v2); gated by env")
        print("✅ retriever.search(query) returns [{chunk, score, source, url}] with length ≤ K")
        
        return 0
    else:
        print("\n⚠️  Some DoD requirements not satisfied. Check implementation.")
        return 1


if __name__ == "__main__":
    sys.exit(main())
