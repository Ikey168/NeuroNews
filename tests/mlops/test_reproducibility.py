"""
Tests for MLflow Reproducibility Framework

Issue #222: Tests for environment capture and data manifest generation
to ensure reproducibility features work correctly.
"""

import json
import os
import shutil
import tempfile
import unittest
from pathlib import Path
from unittest.mock import patch, MagicMock

import sys
sys.path.append(os.path.join(os.path.dirname(__file__), "../.."))

from scripts.capture_env import capture_environment, get_pip_packages
from services.mlops.data_manifest import DataManifestGenerator


class TestEnvironmentCapture(unittest.TestCase):
    """Test environment capture functionality."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.temp_dir = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, self.temp_dir)
    
    def test_capture_environment(self):
        """Test basic environment capture."""
        artifacts = capture_environment(
            output_dir=self.temp_dir,
            log_to_mlflow=False
        )
        
        # Check that artifacts were created
        self.assertIn("requirements.txt", artifacts)
        self.assertIn("system_info.json", artifacts)
        
        # Check files exist
        for artifact_path in artifacts.values():
            self.assertTrue(Path(artifact_path).exists())
        
        # Check requirements.txt format
        requirements_path = artifacts["requirements.txt"]
        with open(requirements_path, 'r') as f:
            content = f.read()
            self.assertIn("# Generated by capture_env.py", content)
            self.assertIn("# Python", content)
        
        # Check system_info.json structure
        system_info_path = artifacts["system_info.json"]
        with open(system_info_path, 'r') as f:
            system_info = json.load(f)
            
            required_keys = ['timestamp', 'platform', 'python', 'cpu', 'memory', 'gpu', 'git']
            for key in required_keys:
                self.assertIn(key, system_info)
    
    def test_get_pip_packages(self):
        """Test pip packages extraction."""
        packages = get_pip_packages()
        
        # Should be a dictionary
        self.assertIsInstance(packages, dict)
        
        # Should contain some common packages
        # (exact packages depend on environment, so just check structure)
        if packages:  # If any packages are installed
            for name, version in packages.items():
                self.assertIsInstance(name, str)
                self.assertIsInstance(version, str)
    
    @patch('mlflow.log_artifact')
    @patch('mlflow.log_param')
    @patch('mlflow.log_metric')
    def test_capture_environment_mlflow_logging(self, mock_log_metric, mock_log_param, mock_log_artifact):
        """Test MLflow logging integration."""
        artifacts = capture_environment(
            output_dir=self.temp_dir,
            log_to_mlflow=True
        )
        
        # Check that MLflow logging methods were called
        self.assertTrue(mock_log_artifact.called)
        self.assertTrue(mock_log_param.called)
        
        # Check specific parameters were logged
        logged_params = {call[0][0]: call[0][1] for call in mock_log_param.call_args_list}
        expected_params = ['python_version', 'platform_system', 'cpu_count', 'gpu_count']
        
        for param in expected_params:
            self.assertIn(param, logged_params)


class TestDataManifestGenerator(unittest.TestCase):
    """Test data manifest generation functionality."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.temp_dir = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, self.temp_dir)
        
        # Create test files
        self.test_files = []
        for i in range(3):
            file_path = Path(self.temp_dir) / f"test_file_{i}.txt"
            with open(file_path, 'w') as f:
                f.write(f"Test content for file {i}\n" * (i + 1))
            self.test_files.append(file_path)
        
        # Create subdirectory with file
        sub_dir = Path(self.temp_dir) / "subdir"
        sub_dir.mkdir()
        sub_file = sub_dir / "sub_file.txt"
        with open(sub_file, 'w') as f:
            f.write("Subdirectory file content")
        self.test_files.append(sub_file)
        
        self.generator = DataManifestGenerator(hash_algorithms=['md5', 'sha256'])
    
    def test_calculate_file_hash(self):
        """Test file hash calculation."""
        test_file = self.test_files[0]
        
        # Test MD5
        md5_hash = self.generator.calculate_file_hash(test_file, 'md5')
        self.assertIsInstance(md5_hash, str)
        self.assertEqual(len(md5_hash), 32)  # MD5 is 32 hex chars
        
        # Test SHA256
        sha256_hash = self.generator.calculate_file_hash(test_file, 'sha256')
        self.assertIsInstance(sha256_hash, str)
        self.assertEqual(len(sha256_hash), 64)  # SHA256 is 64 hex chars
        
        # Test consistency
        md5_hash2 = self.generator.calculate_file_hash(test_file, 'md5')
        self.assertEqual(md5_hash, md5_hash2)
    
    def test_get_file_metadata(self):
        """Test file metadata extraction."""
        test_file = self.test_files[0]
        metadata = self.generator.get_file_metadata(test_file)
        
        # Check required fields
        required_fields = [
            'path', 'name', 'size_bytes', 'size_human', 
            'modified_time', 'created_time', 'is_file', 'hashes'
        ]
        
        for field in required_fields:
            self.assertIn(field, metadata)
        
        # Check hash structure
        self.assertIn('md5', metadata['hashes'])
        self.assertIn('sha256', metadata['hashes'])
        
        # Check file properties
        self.assertTrue(metadata['is_file'])
        self.assertFalse(metadata['is_directory'])
        self.assertEqual(metadata['name'], test_file.name)
    
    def test_scan_directory(self):
        """Test directory scanning."""
        files = self.generator.scan_directory(self.temp_dir)
        
        # Should find all test files
        self.assertEqual(len(files), len(self.test_files))
        
        # Test include patterns
        txt_files = self.generator.scan_directory(
            self.temp_dir, 
            include_patterns=['**/*.txt']
        )
        self.assertEqual(len(txt_files), len(self.test_files))
        
        # Test exclude patterns
        main_files = self.generator.scan_directory(
            self.temp_dir,
            exclude_patterns=['**/subdir/**']
        )
        self.assertEqual(len(main_files), 3)  # Excludes subdirectory file
    
    def test_generate_manifest(self):
        """Test manifest generation."""
        manifest = self.generator.generate_manifest(self.temp_dir)
        
        # Check manifest structure
        required_keys = [
            'manifest_version', 'generated_at', 'data_path', 
            'hash_algorithms', 'files', 'summary'
        ]
        
        for key in required_keys:
            self.assertIn(key, manifest)
        
        # Check files were processed
        self.assertEqual(len(manifest['files']), len(self.test_files))
        
        # Check summary
        summary = manifest['summary']
        self.assertEqual(summary['total_files'], len(self.test_files))
        self.assertGreater(summary['total_size_bytes'], 0)
        self.assertGreater(summary['processing_time_seconds'], 0)
        
        # Check file details
        for file_info in manifest['files']:
            self.assertIn('path', file_info)
            self.assertIn('size_bytes', file_info)
            self.assertIn('hashes', file_info)
            self.assertIn('md5', file_info['hashes'])
            self.assertIn('sha256', file_info['hashes'])
    
    def test_save_manifest(self):
        """Test manifest saving."""
        manifest = self.generator.generate_manifest(self.temp_dir)
        
        manifest_path = Path(self.temp_dir) / "test_manifest.json"
        saved_path = self.generator.save_manifest(manifest, manifest_path)
        
        # Check file was created
        self.assertTrue(saved_path.exists())
        self.assertEqual(saved_path, manifest_path)
        
        # Check content
        with open(manifest_path, 'r') as f:
            loaded_manifest = json.load(f)
        
        self.assertEqual(loaded_manifest['manifest_version'], manifest['manifest_version'])
        self.assertEqual(len(loaded_manifest['files']), len(manifest['files']))
    
    def test_validate_manifest(self):
        """Test manifest validation."""
        # Generate original manifest
        original_manifest = self.generator.generate_manifest(self.temp_dir)
        
        # Validate against unchanged data (should pass)
        validation_results = self.generator.validate_manifest(
            original_manifest, self.temp_dir
        )
        
        self.assertTrue(validation_results['valid'])
        self.assertEqual(validation_results['found_files'], len(self.test_files))
        self.assertEqual(len(validation_results['missing_files']), 0)
        self.assertEqual(len(validation_results['hash_mismatches']), 0)
        
        # Modify a file
        test_file = self.test_files[0]
        with open(test_file, 'a') as f:
            f.write("\nModified content")
        
        # Validate against modified data (should fail)
        validation_results = self.generator.validate_manifest(
            original_manifest, self.temp_dir
        )
        
        self.assertFalse(validation_results['valid'])
        self.assertGreater(len(validation_results['hash_mismatches']), 0)
        
        # Check mismatch details
        mismatch = validation_results['hash_mismatches'][0]
        self.assertIn('file', mismatch)
        self.assertIn('algorithm', mismatch)
        self.assertIn('expected', mismatch)
        self.assertIn('actual', mismatch)
        self.assertNotEqual(mismatch['expected'], mismatch['actual'])
    
    @patch('mlflow.log_artifact')
    @patch('mlflow.log_metric')
    @patch('mlflow.log_param')
    def test_log_to_mlflow(self, mock_log_param, mock_log_metric, mock_log_artifact):
        """Test MLflow logging integration."""
        manifest = self.generator.generate_manifest(self.temp_dir)
        
        self.generator.log_to_mlflow(manifest, "test_manifest")
        
        # Check that MLflow methods were called
        self.assertTrue(mock_log_artifact.called)
        self.assertTrue(mock_log_metric.called)
        self.assertTrue(mock_log_param.called)
        
        # Check specific metrics were logged
        logged_metrics = {call[0][0]: call[0][1] for call in mock_log_metric.call_args_list}
        expected_metrics = ['data_files_count', 'data_size_bytes', 'data_processing_time']
        
        for metric in expected_metrics:
            self.assertIn(metric, logged_metrics)
        
        # Check specific parameters were logged
        logged_params = {call[0][0]: call[0][1] for call in mock_log_param.call_args_list}
        expected_params = ['data_path', 'hash_algorithms', 'manifest_version']
        
        for param in expected_params:
            self.assertIn(param, logged_params)


class TestReproducibilityIntegration(unittest.TestCase):
    """Test integration between environment capture and data manifest."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.temp_dir = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, self.temp_dir)
    
    @patch('mlflow.log_artifact')
    @patch('mlflow.log_metric')
    @patch('mlflow.log_param')
    def test_full_reproducibility_capture(self, mock_log_param, mock_log_metric, mock_log_artifact):
        """Test complete reproducibility capture workflow."""
        # Create test data
        data_file = Path(self.temp_dir) / "test_data.csv"
        with open(data_file, 'w') as f:
            f.write("col1,col2,col3\n1,2,3\n4,5,6\n")
        
        # Capture environment
        env_artifacts = capture_environment(
            output_dir=self.temp_dir,
            log_to_mlflow=True
        )
        
        # Generate data manifest
        generator = DataManifestGenerator()
        manifest = generator.generate_manifest(
            self.temp_dir,
            metadata={"test": "integration"}
        )
        generator.log_to_mlflow(manifest)
        
        # Check that both components logged to MLflow
        self.assertTrue(mock_log_artifact.called)
        self.assertTrue(mock_log_param.called)
        
        # Check artifacts were created
        self.assertIn("requirements.txt", env_artifacts)
        self.assertIn("system_info.json", env_artifacts)
        
        # Check manifest was generated
        self.assertIn("files", manifest)
        self.assertGreater(len(manifest["files"]), 0)
    
    def test_reproducibility_validation_workflow(self):
        """Test complete validation workflow."""
        # Create test data
        data_file = Path(self.temp_dir) / "test_data.json"
        test_data = {"key": "value", "numbers": [1, 2, 3]}
        with open(data_file, 'w') as f:
            json.dump(test_data, f)
        
        # Initial capture
        env_artifacts = capture_environment(
            output_dir=self.temp_dir,
            log_to_mlflow=False
        )
        
        generator = DataManifestGenerator()
        original_manifest = generator.generate_manifest(self.temp_dir)
        
        # Save artifacts for later validation
        manifest_path = Path(self.temp_dir) / "original_manifest.json"
        generator.save_manifest(original_manifest, manifest_path)
        
        # Simulate later validation
        validation_results = generator.validate_manifest(
            original_manifest, self.temp_dir
        )
        
        self.assertTrue(validation_results['valid'])
        
        # Modify data and re-validate
        with open(data_file, 'w') as f:
            json.dump({"modified": True}, f)
        
        validation_results = generator.validate_manifest(
            original_manifest, self.temp_dir
        )
        
        self.assertFalse(validation_results['valid'])
        self.assertGreater(len(validation_results['hash_mismatches']), 0)


if __name__ == '__main__':
    # Set up test environment
    os.environ['MLFLOW_TRACKING_URI'] = 'file:./test_mlruns'
    
    # Run tests
    unittest.main(verbosity=2)
