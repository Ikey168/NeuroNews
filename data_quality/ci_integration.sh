#!/bin/bash
set -e

# Great Expectations CI Integration Script
# This script runs data quality checks and provides CI feedback

# Configuration
EXPECTATIONS_DIR="data_quality/expectations"
RESULTS_DIR="data_quality/results"
GX_DIR="data_quality/gx"
DATA_FILE="${1:-}"
TABLE_NAME="${2:-}"
CONNECTION_STRING="${3:-}"
SCHEMA_NAME="${4:-}"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Logging functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Function to install dependencies
install_dependencies() {
    log_info "Installing Great Expectations and dependencies..."
    pip install -q great-expectations pandas sqlalchemy psycopg2-binary
}

# Function to run data quality checks
run_quality_checks() {
    local data_source="$1"
    local output_file="${RESULTS_DIR}/ci_results_$(date +%Y%m%d_%H%M%S).json"
    
    log_info "Running data quality checks..."
    
    # Ensure results directory exists
    mkdir -p "$RESULTS_DIR"
    
    # Run checks based on data source type
    if [ -n "$data_source" ] && [ -f "$data_source" ]; then
        # CSV file validation
        log_info "Validating CSV file: $data_source"
        python data_quality/checks.py \
            --data-file "$data_source" \
            --output-format json \
            --fail-on-error > "$output_file"
    elif [ -n "$TABLE_NAME" ] && [ -n "$CONNECTION_STRING" ]; then
        # Database table validation
        log_info "Validating database table: $TABLE_NAME"
        python data_quality/checks.py \
            --table "$TABLE_NAME" \
            --connection-string "$CONNECTION_STRING" \
            ${SCHEMA_NAME:+--schema "$SCHEMA_NAME"} \
            --suite articles \
            --output-format json \
            --fail-on-error > "$output_file"
    else
        log_error "No valid data source provided. Use CSV file or database table."
        exit 1
    fi
    
    echo "$output_file"
}

# Function to generate PR annotation
generate_pr_annotation() {
    local results_file="$1"
    
    if [ ! -f "$results_file" ]; then
        log_error "Results file not found: $results_file"
        return 1
    fi
    
    log_info "Generating PR annotation..."
    
    # Parse results and create annotation
    python -c "
import json
import sys

with open('$results_file', 'r') as f:
    results = json.load(f)

def format_results(data):
    if isinstance(data, dict) and 'overall_success' in data:
        # Single suite results
        suites = {'single_suite': data}
    else:
        # Multiple suite results
        suites = data
    
    total_suites = len(suites)
    passed_suites = sum(1 for suite_data in suites.values() if suite_data.get('overall_success', False))
    
    annotation = f'## 📊 Data Quality Report\n\n'
    annotation += f'**Overall Status**: {"✅ PASSED" if passed_suites == total_suites else "❌ FAILED"}\n'
    annotation += f'**Suites Passed**: {passed_suites}/{total_suites}\n\n'
    
    for suite_name, suite_data in suites.items():
        if suite_data.get('error'):
            annotation += f'### ❌ {suite_name}\n'
            annotation += f'**Error**: {suite_data["error"]}\n\n'
        else:
            success = suite_data.get('overall_success', False)
            success_percent = suite_data.get('success_percent', 0)
            failed_count = suite_data.get('failed_expectations_count', 0)
            
            status_icon = '✅' if success else '❌'
            annotation += f'### {status_icon} {suite_name}\n'
            annotation += f'**Success Rate**: {success_percent:.1f}%\n'
            
            if failed_count > 0:
                annotation += f'**Failed Expectations**: {failed_count}\n'
                
                # Show first few failed expectations
                failed_expectations = suite_data.get('failed_expectations', [])[:3]
                for fail in failed_expectations:
                    exp_type = fail.get('expectation_type', 'Unknown')
                    column = fail.get('column', 'N/A')
                    annotation += f'- {exp_type} on column \`{column}\`\n'
                
                if len(suite_data.get('failed_expectations', [])) > 3:
                    remaining = len(suite_data.get('failed_expectations', [])) - 3
                    annotation += f'- ... and {remaining} more\n'
            
            annotation += '\n'
    
    annotation += f'---\n'
    annotation += f'*Generated by Great Expectations at {suite_data.get("timestamp", "unknown time")}*\n'
    
    return annotation

try:
    annotation = format_results(results)
    print(annotation)
except Exception as e:
    print(f'Error generating annotation: {e}', file=sys.stderr)
    sys.exit(1)
"
}

# Function to create dashboard summary
create_dashboard_summary() {
    local results_file="$1"
    local dashboard_file="${RESULTS_DIR}/dashboard_summary.json"
    
    log_info "Creating dashboard summary..."
    
    python -c "
import json
import os
from datetime import datetime

results_file = '$results_file'
dashboard_file = '$dashboard_file'

# Load current results
with open(results_file, 'r') as f:
    current_results = json.load(f)

# Load existing dashboard data or create new
if os.path.exists(dashboard_file):
    with open(dashboard_file, 'r') as f:
        dashboard_data = json.load(f)
else:
    dashboard_data = {
        'runs': [],
        'summary': {
            'total_runs': 0,
            'successful_runs': 0,
            'last_updated': None
        }
    }

# Process current results
if isinstance(current_results, dict) and 'overall_success' in current_results:
    # Single suite results
    suites_data = {'single_suite': current_results}
else:
    # Multiple suite results
    suites_data = current_results

# Calculate aggregated metrics
total_suites = len(suites_data)
passed_suites = sum(1 for suite_data in suites_data.values() if suite_data.get('overall_success', False))
overall_success = passed_suites == total_suites

# Add to dashboard history
run_entry = {
    'timestamp': datetime.now().isoformat(),
    'overall_success': overall_success,
    'total_suites': total_suites,
    'passed_suites': passed_suites,
    'success_rate': (passed_suites / total_suites * 100) if total_suites > 0 else 0,
    'suites': {name: data.get('success_percent', 0) for name, data in suites_data.items()}
}

dashboard_data['runs'].append(run_entry)

# Update summary
dashboard_data['summary']['total_runs'] += 1
if overall_success:
    dashboard_data['summary']['successful_runs'] += 1
dashboard_data['summary']['last_updated'] = datetime.now().isoformat()
dashboard_data['summary']['overall_pass_rate'] = (
    dashboard_data['summary']['successful_runs'] / dashboard_data['summary']['total_runs'] * 100
)

# Keep only last 100 runs
dashboard_data['runs'] = dashboard_data['runs'][-100:]

# Save dashboard data
os.makedirs(os.path.dirname(dashboard_file), exist_ok=True)
with open(dashboard_file, 'w') as f:
    json.dump(dashboard_data, f, indent=2)

print(f'Dashboard updated: {dashboard_data[\"summary\"][\"overall_pass_rate\"]:.1f}% pass rate over {dashboard_data[\"summary\"][\"total_runs\"]} runs')
"
}

# Function to check if PR should be blocked
should_block_pr() {
    local results_file="$1"
    
    python -c "
import json
import sys

with open('$results_file', 'r') as f:
    results = json.load(f)

def check_blocking_conditions(data):
    if isinstance(data, dict) and 'overall_success' in data:
        # Single suite results
        return not data.get('overall_success', False)
    else:
        # Multiple suite results
        for suite_data in data.values():
            if not suite_data.get('overall_success', False):
                return True
    return False

if check_blocking_conditions(results):
    sys.exit(1)  # Block PR
else:
    sys.exit(0)  # Allow PR
"
}

# Main execution
main() {
    log_info "Starting Great Expectations CI integration..."
    
    # Install dependencies if needed
    if ! python -c "import great_expectations" 2>/dev/null; then
        install_dependencies
    fi
    
    # Run quality checks
    results_file=$(run_quality_checks "$DATA_FILE")
    
    if [ $? -eq 0 ]; then
        log_success "Data quality checks completed successfully"
        
        # Generate PR annotation
        annotation=$(generate_pr_annotation "$results_file")
        echo "$annotation"
        
        # Create dashboard summary
        create_dashboard_summary "$results_file"
        
        # Check if PR should be blocked
        if should_block_pr "$results_file"; then
            log_error "Data quality checks failed - blocking PR"
            exit 1
        else
            log_success "All data quality checks passed - PR can proceed"
            exit 0
        fi
    else
        log_error "Data quality checks failed"
        exit 1
    fi
}

# Usage information
usage() {
    echo "Usage: $0 [DATA_FILE] [TABLE_NAME] [CONNECTION_STRING] [SCHEMA_NAME]"
    echo ""
    echo "Examples:"
    echo "  $0 data/articles.csv                    # Validate CSV file"
    echo "  $0 '' articles postgresql://...         # Validate database table"
    echo "  $0 '' articles postgresql://... public  # Validate table with schema"
    echo ""
    echo "Environment variables:"
    echo "  CI                 - Set to 'true' in CI environment"
    echo "  GITHUB_TOKEN       - GitHub token for PR annotations"
    echo "  DATABASE_URL       - Default database connection string"
}

# Check for help flag
if [ "$1" = "-h" ] || [ "$1" = "--help" ]; then
    usage
    exit 0
fi

# Run main function
main "$@"
