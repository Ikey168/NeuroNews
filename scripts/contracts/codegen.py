#!/usr/bin/env python3
"""
Code generation script for data contracts.

This script generates Python types (Pydantic models and dataclasses) from
Avro schemas and JSON schemas to prevent schema drift and ensure type safety.
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Union

import argparse
import re


class TypeGenerator:
    """Base class for type generation."""
    
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def generate_header(self, schema_name: str, schema_type: str) -> str:
        """Generate file header with metadata."""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        return f'''"""
Auto-generated types from {schema_type} schema: {schema_name}
Generated at: {timestamp}

DO NOT EDIT THIS FILE MANUALLY!
This file is auto-generated from contract schemas.
To make changes, update the schema file and regenerate.
"""

from __future__ import annotations

import json
from datetime import datetime
from typing import Any, Dict, List, Optional, Union
from pydantic import BaseModel, Field, validator
from dataclasses import dataclass

'''

    def sanitize_name(self, name: str) -> str:
        """Sanitize field/class names for Python."""
        # Convert to PascalCase for class names, snake_case for fields
        name = re.sub(r'[^a-zA-Z0-9_]', '_', name)
        name = re.sub(r'_{2,}', '_', name)
        name = name.strip('_')
        
        # Avoid Python keywords
        if name in {'class', 'def', 'if', 'else', 'for', 'while', 'try', 'except', 'import', 'from', 'as', 'with', 'pass', 'break', 'continue', 'return', 'yield', 'lambda', 'global', 'nonlocal', 'assert', 'del', 'and', 'or', 'not', 'in', 'is'}:
            name = f"{name}_field"
            
        return name

    def to_pascal_case(self, name: str) -> str:
        """Convert to PascalCase for class names."""
        name = self.sanitize_name(name)
        return ''.join(word.capitalize() for word in name.split('_'))

    def to_snake_case(self, name: str) -> str:
        """Convert to snake_case for field names."""
        name = self.sanitize_name(name)
        # Convert camelCase to snake_case
        name = re.sub(r'([a-z0-9])([A-Z])', r'\1_\2', name)
        return name.lower()


class AvroTypeGenerator(TypeGenerator):
    """Generate Python types from Avro schemas."""
    
    def __init__(self, output_dir: Path):
        super().__init__(output_dir)
        self.type_mapping = {
            'string': 'str',
            'int': 'int', 
            'long': 'int',
            'float': 'float',
            'double': 'float',
            'boolean': 'bool',
            'bytes': 'bytes',
            'null': 'None'
        }
        
    def map_avro_type(self, avro_type: Union[str, Dict, List]) -> str:
        """Map Avro type to Python type annotation."""
        if isinstance(avro_type, str):
            return self.type_mapping.get(avro_type, avro_type)
        
        elif isinstance(avro_type, list):
            # Union type
            types = [self.map_avro_type(t) for t in avro_type]
            if 'None' in types:
                # Optional type
                non_null_types = [t for t in types if t != 'None']
                if len(non_null_types) == 1:
                    return f"Optional[{non_null_types[0]}]"
                else:
                    return f"Optional[Union[{', '.join(non_null_types)}]]"
            else:
                return f"Union[{', '.join(types)}]"
        
        elif isinstance(avro_type, dict):
            type_name = avro_type.get('type')
            
            if type_name == 'array':
                item_type = self.map_avro_type(avro_type['items'])
                return f"List[{item_type}]"
            
            elif type_name == 'map':
                value_type = self.map_avro_type(avro_type['values'])
                return f"Dict[str, {value_type}]"
            
            elif type_name == 'record':
                # Custom record type
                return self.to_pascal_case(avro_type['name'])
            
            elif type_name == 'enum':
                # Enum type - generate as Literal
                values = avro_type['symbols']
                return f"Literal[{', '.join(repr(v) for v in values)}]"
            
            elif type_name in ['long', 'int', 'float', 'double', 'string', 'boolean', 'bytes']:
                # Check for logical types
                logical_type = avro_type.get('logicalType')
                if logical_type == 'timestamp-millis':
                    return 'datetime'
                elif logical_type == 'date':
                    return 'date'
                elif logical_type == 'time-millis':
                    return 'time'
                else:
                    return self.type_mapping.get(type_name, type_name)
        
        return 'Any'

    def generate_pydantic_model(self, schema: Dict[str, Any]) -> str:
        """Generate Pydantic model from Avro record schema."""
        if schema.get('type') != 'record':
            raise ValueError("Only record types can be converted to Pydantic models")
        
        class_name = self.to_pascal_case(schema['name'])
        fields = schema.get('fields', [])
        
        # Generate imports for special types
        imports = set()
        if any('timestamp-millis' in str(field) for field in fields):
            imports.add('from datetime import datetime')
        if any('date' in str(field) for field in fields):
            imports.add('from datetime import date')
        if any('time' in str(field) for field in fields):
            imports.add('from datetime import time')
        
        # Check if we need Literal import
        if any(isinstance(field.get('type'), dict) and field['type'].get('type') == 'enum' for field in fields):
            imports.add('from typing import Literal')
        
        imports_str = '\n'.join(sorted(imports)) + '\n' if imports else ''
        
        # Generate class docstring
        doc = schema.get('doc', f'Pydantic model for {class_name}')
        
        # Generate field definitions
        field_defs = []
        for field in fields:
            field_name = self.to_snake_case(field['name'])
            field_type = self.map_avro_type(field['type'])
            field_doc = field.get('doc', '')
            default_value = field.get('default')
            
            # Handle default values
            if default_value is not None:
                if isinstance(default_value, str):
                    default_str = f'= "{default_value}"'
                elif isinstance(default_value, list):
                    default_str = f'= {default_value}'
                elif default_value is None:
                    default_str = '= None'
                else:
                    default_str = f'= {default_value}'
            elif 'null' in str(field['type']):
                default_str = '= None'
            else:
                default_str = ''
            
            # Generate Field with description
            if field_doc:
                field_def = f'    {field_name}: {field_type} = Field({default_str[2:] if default_str else "..."}, description="{field_doc}")'
            else:
                if default_str:
                    field_def = f'    {field_name}: {field_type} {default_str}'
                else:
                    field_def = f'    {field_name}: {field_type}'
            
            field_defs.append(field_def)
        
        model_str = f'''
{imports_str}
class {class_name}(BaseModel):
    """{doc}"""
    
{chr(10).join(field_defs)}
    
    class Config:
        """Pydantic model configuration."""
        allow_population_by_field_name = True
        json_encoders = {{
            datetime: lambda v: int(v.timestamp() * 1000),  # Convert to milliseconds
        }}
        
    @classmethod
    def from_avro_dict(cls, data: Dict[str, Any]) -> '{class_name}':
        """Create instance from Avro dictionary."""
        return cls(**data)
    
    def to_avro_dict(self) -> Dict[str, Any]:
        """Convert to Avro-compatible dictionary."""
        return self.dict()
'''
        
        return model_str

    def generate_dataclass(self, schema: Dict[str, Any]) -> str:
        """Generate dataclass from Avro record schema."""
        if schema.get('type') != 'record':
            raise ValueError("Only record types can be converted to dataclasses")
        
        class_name = self.to_pascal_case(schema['name'])
        fields = schema.get('fields', [])
        
        # Generate imports for special types
        imports = set()
        if any('timestamp-millis' in str(field) for field in fields):
            imports.add('from datetime import datetime')
        
        imports_str = '\n'.join(sorted(imports)) + '\n' if imports else ''
        
        # Generate field definitions
        field_defs = []
        for field in fields:
            field_name = self.to_snake_case(field['name'])
            field_type = self.map_avro_type(field['type'])
            default_value = field.get('default')
            
            if default_value is not None:
                if isinstance(default_value, str):
                    default_str = f' = "{default_value}"'
                elif default_value is None:
                    default_str = ' = None'
                else:
                    default_str = f' = {default_value}'
            elif 'Optional' in field_type:
                default_str = ' = None'
            else:
                default_str = ''
            
            field_def = f'    {field_name}: {field_type}{default_str}'
            field_defs.append(field_def)
        
        doc = schema.get('doc', f'Dataclass for {class_name}')
        
        dataclass_str = f'''
{imports_str}
@dataclass
class {class_name}:
    """{doc}"""
    
{chr(10).join(field_defs)}
    
    @classmethod
    def from_avro_dict(cls, data: Dict[str, Any]) -> '{class_name}':
        """Create instance from Avro dictionary."""
        return cls(**data)
    
    def to_avro_dict(self) -> Dict[str, Any]:
        """Convert to Avro-compatible dictionary."""
        import dataclasses
        return dataclasses.asdict(self)
'''
        
        return dataclass_str

    def generate_from_schema(self, schema_path: Path, generate_dataclass: bool = False) -> str:
        """Generate Python code from Avro schema file."""
        with open(schema_path, 'r') as f:
            schema = json.load(f)
        
        schema_name = schema_path.stem.replace('-', '_')  # Use underscores for Python
        header = self.generate_header(schema_name, "Avro")
        
        if generate_dataclass:
            type_code = self.generate_dataclass(schema)
        else:
            type_code = self.generate_pydantic_model(schema)
        
        return header + type_code


class JsonSchemaTypeGenerator(TypeGenerator):
    """Generate Python types from JSON schemas."""
    
    def __init__(self, output_dir: Path):
        super().__init__(output_dir)
        self.type_mapping = {
            'string': 'str',
            'integer': 'int',
            'number': 'float',
            'boolean': 'bool',
            'array': 'List',
            'object': 'Dict[str, Any]',
            'null': 'None'
        }

    def map_json_type(self, json_type: Union[str, List], property_def: Dict[str, Any]) -> str:
        """Map JSON Schema type to Python type annotation."""
        if isinstance(json_type, list):
            # Union of types
            types = [self.map_json_type(t, property_def) for t in json_type]
            if 'None' in types:
                non_null_types = [t for t in types if t != 'None']
                if len(non_null_types) == 1:
                    return f"Optional[{non_null_types[0]}]"
                else:
                    return f"Optional[Union[{', '.join(non_null_types)}]]"
            else:
                return f"Union[{', '.join(types)}]"
        
        elif json_type == 'array':
            items = property_def.get('items', {})
            if isinstance(items, dict):
                item_type = self.map_json_type(items.get('type', 'Any'), items)
                return f"List[{item_type}]"
            else:
                return "List[Any]"
        
        elif json_type == 'object':
            properties = property_def.get('properties')
            if properties:
                # Could generate a nested class, but for simplicity use Dict
                return "Dict[str, Any]"
            else:
                return "Dict[str, Any]"
        
        elif json_type == 'string':
            # Check for format hints
            format_type = property_def.get('format')
            if format_type == 'date-time':
                return 'datetime'
            elif format_type == 'date':
                return 'date'
            elif format_type == 'time':
                return 'time'
            elif format_type == 'uri':
                return 'str'  # Could use pydantic.HttpUrl
            
            # Check for enum
            enum_values = property_def.get('enum')
            if enum_values:
                return f"Literal[{', '.join(repr(v) for v in enum_values)}]"
            
            return 'str'
        
        return self.type_mapping.get(json_type, 'Any')

    def generate_pydantic_model(self, schema: Dict[str, Any]) -> str:
        """Generate Pydantic model from JSON schema."""
        title = schema.get('title', 'GeneratedModel')
        class_name = self.to_pascal_case(title)
        description = schema.get('description', f'Pydantic model for {title}')
        
        properties = schema.get('properties', {})
        required_fields = set(schema.get('required', []))
        
        # Generate imports for special types
        imports = set()
        datetime_used = False
        literal_used = False
        
        # Generate field definitions
        field_defs = []
        for prop_name, prop_def in properties.items():
            field_name = self.to_snake_case(prop_name)
            prop_type = prop_def.get('type', 'string')
            python_type = self.map_json_type(prop_type, prop_def)
            field_desc = prop_def.get('description', '')
            default_value = prop_def.get('default')
            
            # Track imports needed
            if 'datetime' in python_type:
                datetime_used = True
            if 'Literal' in python_type:
                literal_used = True
            
            # Handle required vs optional fields
            is_required = prop_name in required_fields
            
            # Generate Field definition
            field_parts = []
            
            if default_value is not None:
                if isinstance(default_value, str):
                    field_parts.append(f'default="{default_value}"')
                else:
                    field_parts.append(f'default={default_value}')
            elif not is_required:
                field_parts.append('default=None')
                if not python_type.startswith('Optional'):
                    python_type = f'Optional[{python_type}]'
            
            # Add validation constraints
            if prop_type == 'string':
                min_length = prop_def.get('minLength')
                max_length = prop_def.get('maxLength')
                if min_length is not None:
                    field_parts.append(f'min_length={min_length}')
                if max_length is not None:
                    field_parts.append(f'max_length={max_length}')
            
            elif prop_type in ['integer', 'number']:
                minimum = prop_def.get('minimum')
                maximum = prop_def.get('maximum')
                if minimum is not None:
                    field_parts.append(f'ge={minimum}')
                if maximum is not None:
                    field_parts.append(f'le={maximum}')
            
            if field_desc:
                field_parts.append(f'description="{field_desc}"')
            
            if field_parts:
                field_def = f'    {field_name}: {python_type} = Field({", ".join(field_parts)})'
            else:
                field_def = f'    {field_name}: {python_type}'
            
            field_defs.append(field_def)
        
        # Generate imports
        import_lines = []
        if datetime_used:
            import_lines.append('from datetime import datetime, date, time')
        if literal_used:
            import_lines.append('from typing import Literal')
        
        imports_str = '\n'.join(import_lines) + '\n' if import_lines else ''
        
        model_str = f'''
{imports_str}
class {class_name}(BaseModel):
    """{description}"""
    
{chr(10).join(field_defs) if field_defs else "    pass"}
    
    class Config:
        """Pydantic model configuration."""
        allow_population_by_field_name = True
        extra = "forbid"  # Prevent additional fields
        
    @classmethod
    def from_json_dict(cls, data: Dict[str, Any]) -> '{class_name}':
        """Create instance from JSON dictionary."""
        return cls(**data)
    
    def to_json_dict(self) -> Dict[str, Any]:
        """Convert to JSON-compatible dictionary."""
        return self.dict(exclude_none=True)
'''
        
        return model_str

    def generate_from_schema(self, schema_path: Path) -> str:
        """Generate Python code from JSON schema file."""
        with open(schema_path, 'r') as f:
            schema = json.load(f)
        
        schema_name = schema_path.stem.replace('-', '_')  # Use underscores for Python
        header = self.generate_header(schema_name, "JSON Schema")
        type_code = self.generate_pydantic_model(schema)
        
        return header + type_code


def generate_all_types(contracts_dir: Path, output_dir: Path, include_dataclasses: bool = False):
    """Generate all types from contract schemas."""
    print(f"🔄 Generating types from contracts in {contracts_dir}")
    print(f"   Output directory: {output_dir}")
    
    # Create output directory structure
    avro_output = output_dir / "avro"
    json_output = output_dir / "jsonschema"
    avro_output.mkdir(parents=True, exist_ok=True)
    json_output.mkdir(parents=True, exist_ok=True)
    
    # Generate from Avro schemas
    avro_gen = AvroTypeGenerator(avro_output)
    avro_schemas = list((contracts_dir / "schemas" / "avro").glob("**/*.avsc"))
    
    print(f"\n📋 Processing {len(avro_schemas)} Avro schemas...")
    for schema_path in avro_schemas:
        try:
            print(f"   Generating {schema_path.name}...")
            
            # Generate Pydantic models
            pydantic_code = avro_gen.generate_from_schema(schema_path, generate_dataclass=False)
            safe_name = schema_path.stem.replace('-', '_')  # Use underscores for filenames
            pydantic_file = avro_output / f"{safe_name}_models.py"
            with open(pydantic_file, 'w') as f:
                f.write(pydantic_code)
            
            # Optionally generate dataclasses
            if include_dataclasses:
                dataclass_code = avro_gen.generate_from_schema(schema_path, generate_dataclass=True)
                dataclass_file = avro_output / f"{safe_name}_dataclasses.py"
                with open(dataclass_file, 'w') as f:
                    f.write(dataclass_code)
            
            print(f"   ✓ Generated {pydantic_file.name}")
            
        except Exception as e:
            print(f"   ❌ Error processing {schema_path.name}: {e}")
    
    # Generate from JSON schemas
    json_gen = JsonSchemaTypeGenerator(json_output)
    json_schemas = list((contracts_dir / "schemas" / "jsonschema").glob("**/*.json"))
    
    print(f"\n📋 Processing {len(json_schemas)} JSON schemas...")
    for schema_path in json_schemas:
        try:
            print(f"   Generating {schema_path.name}...")
            
            code = json_gen.generate_from_schema(schema_path)
            safe_name = schema_path.stem.replace('-', '_')  # Use underscores for filenames
            output_file = json_output / f"{safe_name}_models.py"
            with open(output_file, 'w') as f:
                f.write(code)
            
            print(f"   ✓ Generated {output_file.name}")
            
        except Exception as e:
            print(f"   ❌ Error processing {schema_path.name}: {e}")
    
    # Generate __init__.py files
    generate_init_files(output_dir)
    
    print(f"\n🎉 Type generation complete!")
    print(f"   Generated files in: {output_dir}")


def generate_init_files(output_dir: Path):
    """Generate __init__.py files for the generated modules."""
    
    # Root __init__.py
    root_init = output_dir / "__init__.py"
    with open(root_init, 'w') as f:
        f.write('"""Auto-generated types from data contracts."""\n\n')
        f.write('# Import all generated models for easy access\n')
        f.write('from .avro import *\n')
        f.write('from .jsonschema import *\n')
    
    # Avro __init__.py
    avro_dir = output_dir / "avro"
    if avro_dir.exists():
        avro_init = avro_dir / "__init__.py"
        with open(avro_init, 'w') as f:
            f.write('"""Generated types from Avro schemas."""\n\n')
            for model_file in avro_dir.glob("*_models.py"):
                module_name = model_file.stem
                # Replace hyphens with underscores for valid Python identifiers
                safe_module_name = module_name.replace('-', '_')
                f.write(f'from .{safe_module_name} import *\n')
    
    # JSON Schema __init__.py
    json_dir = output_dir / "jsonschema"
    if json_dir.exists():
        json_init = json_dir / "__init__.py"
        with open(json_init, 'w') as f:
            f.write('"""Generated types from JSON schemas."""\n\n')
            for model_file in json_dir.glob("*_models.py"):
                module_name = model_file.stem
                # Replace hyphens with underscores for valid Python identifiers
                safe_module_name = module_name.replace('-', '_')
                f.write(f'from .{safe_module_name} import *\n')


def main():
    """Main entry point for the code generation script."""
    parser = argparse.ArgumentParser(
        description="Generate Python types from data contract schemas"
    )
    parser.add_argument(
        "--contracts-dir",
        type=Path,
        default=Path("contracts"),
        help="Directory containing contract schemas (default: contracts)"
    )
    parser.add_argument(
        "--output-dir", 
        type=Path,
        default=Path("services/generated"),
        help="Output directory for generated types (default: services/generated)"
    )
    parser.add_argument(
        "--include-dataclasses",
        action="store_true",
        help="Also generate dataclasses alongside Pydantic models"
    )
    parser.add_argument(
        "--clean",
        action="store_true", 
        help="Clean output directory before generation"
    )
    
    args = parser.parse_args()
    
    if not args.contracts_dir.exists():
        print(f"❌ Contracts directory not found: {args.contracts_dir}")
        sys.exit(1)
    
    if args.clean and args.output_dir.exists():
        import shutil
        print(f"🧹 Cleaning output directory: {args.output_dir}")
        shutil.rmtree(args.output_dir)
    
    try:
        generate_all_types(
            contracts_dir=args.contracts_dir,
            output_dir=args.output_dir,
            include_dataclasses=args.include_dataclasses
        )
        print("✅ Code generation completed successfully!")
        
    except Exception as e:
        print(f"❌ Code generation failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
