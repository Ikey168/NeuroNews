name: Data Quality Checks

on:
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'data/**'
      - 'contracts/schemas/**'
      - 'services/ingest/**'
      - 'data_quality/**'
  push:
    branches: [ main ]
    paths:
      - 'data/**'
      - 'contracts/schemas/**'  
      - 'services/ingest/**'
      - 'data_quality/**'
  workflow_dispatch:
    inputs:
      data_file:
        description: 'CSV file to validate (optional)'
        required: false
        type: string
      table_name:
        description: 'Database table to validate (optional)'
        required: false
        type: string

jobs:
  data-quality-checks:
    runs-on: ubuntu-latest
    
    permissions:
      contents: read
      pull-requests: write
      issues: read
      
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: neuronews_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
          
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install great-expectations pandas sqlalchemy psycopg2-binary
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        
    - name: Set up test database
      run: |
        # Create test tables based on contracts
        export PGPASSWORD=postgres
        psql -h localhost -U postgres -d neuronews_test -c "
          CREATE TABLE IF NOT EXISTS articles (
            article_id VARCHAR(255) PRIMARY KEY,
            source_id VARCHAR(100) NOT NULL,
            url TEXT NOT NULL,
            title TEXT,
            body TEXT,
            language VARCHAR(2) NOT NULL,
            country VARCHAR(2),
            published_at BIGINT NOT NULL,
            ingested_at BIGINT NOT NULL,
            sentiment_score DOUBLE PRECISION,
            topics TEXT[]
          );
          
          CREATE TABLE IF NOT EXISTS ask_requests (
            id SERIAL PRIMARY KEY,
            question TEXT NOT NULL,
            k INTEGER DEFAULT 5,
            filters_source VARCHAR(100),
            filters_date_from DATE,
            filters_date_to DATE,
            filters_language VARCHAR(2),
            filters_category VARCHAR(50),
            created_at TIMESTAMP DEFAULT NOW()
          );
        "
        
    - name: Generate test data
      run: |
        # Create test CSV files with sample data matching contracts
        mkdir -p test_data
        
        # Articles test data
        cat > test_data/articles.csv << 'EOF'
        article_id,source_id,url,title,body,language,country,published_at,ingested_at,sentiment_score,topics
        test-article-1,reuters,https://reuters.com/test1,Test Article 1,Test body content,en,US,1692892800000,1692893800000,0.5,"[""technology"",""AI""]"
        test-article-2,bbc,https://bbc.com/test2,Test Article 2,Another test body,en,UK,1692892900000,1692893900000,-0.2,"[""politics""]"
        test-article-3,lemonde,https://lemonde.fr/test3,Article en français,Contenu en français,fr,FR,1692893000000,1692894000000,0.8,"[""économie""]"
        EOF
        
        # Ask requests test data  
        cat > test_data/ask_requests.csv << 'EOF'
        question,k,filters_source,filters_date_from,filters_date_to,filters_language,filters_category
        What is the latest news about AI?,5,reuters,2023-08-20,2023-08-25,en,technology
        How is the economy doing?,10,bbc,2023-08-21,2023-08-24,en,economics
        Quelles sont les nouvelles en France?,3,lemonde,2023-08-22,2023-08-23,fr,politics
        EOF
        
    - name: Load test data to database
      run: |
        export PGPASSWORD=postgres
        
        # Load articles data
        psql -h localhost -U postgres -d neuronews_test -c "\copy articles FROM 'test_data/articles.csv' WITH CSV HEADER;"
        
        # Load ask requests data
        psql -h localhost -U postgres -d neuronews_test -c "\copy ask_requests FROM 'test_data/ask_requests.csv' WITH CSV HEADER;"
        
        # Verify data loaded
        echo "Articles count:" $(psql -h localhost -U postgres -d neuronews_test -t -c "SELECT COUNT(*) FROM articles;")
        echo "Ask requests count:" $(psql -h localhost -U postgres -d neuronews_test -t -c "SELECT COUNT(*) FROM ask_requests;")
        
    - name: Run data quality checks on CSV files
      id: csv_checks
      run: |
        echo "::group::CSV File Validation"
        
        # Run checks on articles CSV
        python data_quality/checks.py \
          --data-file test_data/articles.csv \
          --suite articles \
          --output-format json > articles_results.json || echo "Articles validation completed with issues"
          
        # Run checks on ask requests CSV  
        python data_quality/checks.py \
          --data-file test_data/ask_requests.csv \
          --suite ask_requests \
          --output-format json > ask_requests_results.json || echo "Ask requests validation completed with issues"
          
        echo "::endgroup::"
        
    - name: Run data quality checks on database tables
      id: db_checks
      run: |
        echo "::group::Database Table Validation"
        
        export CONNECTION_STRING="postgresql://postgres:postgres@localhost:5432/neuronews_test"
        
        # Run checks on articles table
        python data_quality/checks.py \
          --table articles \
          --connection-string "$CONNECTION_STRING" \
          --suite articles \
          --output-format json > articles_db_results.json || echo "Articles DB validation completed with issues"
          
        # Run checks on ask_requests table
        python data_quality/checks.py \
          --table ask_requests \
          --connection-string "$CONNECTION_STRING" \
          --suite ask_requests \
          --output-format json > ask_requests_db_results.json || echo "Ask requests DB validation completed with issues"
          
        echo "::endgroup::"
        
    - name: Generate CI annotations
      if: always()
      run: |
        echo "::group::Generating PR Annotations"
        
        # Generate annotations for each validation
        if [ -f articles_results.json ]; then
          echo "## 📊 Articles CSV Validation" >> pr_annotation.md
          ./data_quality/ci_integration.sh test_data/articles.csv >> pr_annotation.md || true
        fi
        
        if [ -f ask_requests_results.json ]; then
          echo "## 📊 Ask Requests CSV Validation" >> pr_annotation.md  
          echo "Results available in ask_requests_results.json" >> pr_annotation.md
        fi
        
        if [ -f articles_db_results.json ]; then
          echo "## 📊 Articles Database Validation" >> pr_annotation.md
          echo "Results available in articles_db_results.json" >> pr_annotation.md
        fi
        fi
        
        echo "::endgroup::"
        
    - name: Comment on PR
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          let comment = '# 📊 Data Quality Report\n\n';
          
          if (fs.existsSync('pr_annotation.md')) {
            const annotation = fs.readFileSync('pr_annotation.md', 'utf8');
            comment += annotation;
          } else {
            comment += '⚠️ No data quality results available.\n';
          }
          
          comment += '\n---\n';
          comment += `*Generated by Great Expectations at ${new Date().toISOString()}*\n`;
          comment += `*Workflow: ${context.workflow}, Run: ${context.runNumber}*`;
          
          // Post comment on PR
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
          
    - name: Upload results as artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: data-quality-results
        path: |
          *_results.json
          pr_annotation.md
          data_quality/results/
        retention-days: 30
        
    - name: Update dashboard metrics
      if: always()
      run: |
        echo "::group::Dashboard Metrics Update"
        
        # Calculate overall pass rate
        total_validations=0
        passed_validations=0
        
        for file in *_results.json; do
          if [ -f "$file" ]; then
            total_validations=$((total_validations + 1))
            success=$(python -c 'import json; f=open("'"$file"'"); r=json.load(f); f.close(); print("1" if r.get("overall_success", False) else "0")')
            passed_validations=$((passed_validations + success))
          fi
        done
        
        if [ $total_validations -gt 0 ]; then
          pass_rate=$((passed_validations * 100 / total_validations))
          echo "Overall pass rate: $pass_rate% ($passed_validations/$total_validations)"
          
          # Set output for downstream jobs
          echo "overall_pass_rate=$pass_rate" >> $GITHUB_OUTPUT
          echo "total_validations=$total_validations" >> $GITHUB_OUTPUT
          echo "passed_validations=$passed_validations" >> $GITHUB_OUTPUT
        fi
        
        echo "::endgroup::"
        
    - name: Fail on quality issues
      if: always()
      run: |
        # Check if any validation failed
        failed=false
        
        for file in *_results.json; do
          if [ -f "$file" ]; then
            success=$(python -c 'import json; f=open("'"$file"'"); r=json.load(f); f.close(); print(r.get("overall_success", False))')
            if [ "$success" = "False" ]; then
              echo "❌ Validation failed for $file"
              failed=true
            else
              echo "✅ Validation passed for $file"
            fi
          fi
        done
        
        if [ "$failed" = "true" ]; then
          echo "::error::One or more data quality checks failed"
          exit 1
        else
          echo "::notice::All data quality checks passed"
        fi
