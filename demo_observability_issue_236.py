"""
Demo script for Issue #236: Observability: Prometheus counters + logs

This script demonstrates the Prometheus metrics collection system
for RAG observability and validates all DoD requirements.
"""

import asyncio
import time
import requests
import logging
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def demo_observability():
    """Demonstrate the RAG observability system."""
    print("Issue #236 Demo: Observability: Prometheus counters + logs")
    print("=" * 70)
    
    # Import our metrics system
    try:
        from services.obs.metrics import metrics_collector, demo_metrics, get_metrics
        from services.api.middleware.metrics import demo_middleware
        
        print("âœ… Metrics modules imported successfully")
    except ImportError as e:
        print(f"âŒ Import error: {e}")
        return
    
    print("\nðŸ“Š 1. Basic Metrics Collection")
    print("-" * 40)
    
    # Demo basic metrics collection
    print("Recording sample RAG operations...")
    
    # Record some queries
    metrics_collector.record_query("openai", "en", True, "success")
    metrics_collector.record_query("anthropic", "en", False, "success")
    metrics_collector.record_query("local", "es", True, "error")
    
    # Record latencies
    metrics_collector.record_retrieval_latency(125.5, "openai", 10)
    metrics_collector.record_answer_latency(2500.0, "openai", "en")
    
    # Record answers
    metrics_collector.record_answer("openai", "en", True)
    metrics_collector.record_answer("anthropic", "en", False)
    
    # Record errors
    metrics_collector.record_error("local", "timeout", "retrieval")
    
    # Record document usage
    metrics_collector.record_k_documents(10, "openai", "general")
    metrics_collector.record_k_documents(5, "anthropic", "factual")
    
    print("âœ… Sample metrics recorded")
    
    print("\nðŸ“ˆ 2. Metrics Output")
    print("-" * 40)
    
    # Get metrics in Prometheus format
    metrics_output = get_metrics().decode('utf-8')
    
    # Show relevant metrics
    lines = metrics_output.split('\n')
    
    print("RAG Query Metrics:")
    for line in lines:
        if 'rag_queries_total' in line and not line.startswith('#'):
            print(f"  {line}")
    
    print("\nRAG Answer Metrics:")
    for line in lines:
        if 'rag_answers_total' in line and not line.startswith('#'):
            print(f"  {line}")
    
    print("\nRAG Error Metrics:")
    for line in lines:
        if 'rag_errors_total' in line and not line.startswith('#'):
            print(f"  {line}")
    
    print("\nLatency Histogram Samples:")
    count = 0
    for line in lines:
        if ('rag_retrieval_latency' in line or 'rag_answer_latency' in line) and not line.startswith('#'):
            print(f"  {line}")
            count += 1
            if count >= 5:  # Show first 5 histogram samples
                break
    
    print("\nðŸ—ï¸ 3. File Structure Verification")
    print("-" * 40)
    
    # Check required files
    project_root = Path(__file__).parent.absolute()
    required_files = [
        "services/obs/metrics.py",
        "services/api/middleware/metrics.py", 
        "grafana/dashboards/rag.json"
    ]
    
    for file_path in required_files:
        full_path = project_root / file_path
        if full_path.exists():
            size = full_path.stat().st_size
            print(f"âœ… {file_path} ({size:,} bytes)")
        else:
            print(f"âŒ {file_path} missing")
    
    print("\nðŸŽ¯ 4. DoD Requirements Check")
    print("-" * 40)
    
    print("Counters implemented:")
    counter_checks = {
        "queries_total": "rag_queries_total" in metrics_output,
        "answers_total": "rag_answers_total" in metrics_output,
        "errors_total": "rag_errors_total" in metrics_output
    }
    
    for counter, found in counter_checks.items():
        status = "âœ…" if found else "âŒ"
        print(f"  {status} {counter}")
    
    print("\nHistograms implemented:")
    histogram_checks = {
        "retrieval_latency_ms": "rag_retrieval_latency_milliseconds" in metrics_output,
        "answer_latency_ms": "rag_answer_latency_milliseconds" in metrics_output,
        "k_used": "rag_k_documents_used" in metrics_output
    }
    
    for histogram, found in histogram_checks.items():
        status = "âœ…" if found else "âŒ"
        print(f"  {status} {histogram}")
    
    print("\nLabels implemented:")
    label_checks = {
        "provider": "provider=" in metrics_output,
        "lang": "lang=" in metrics_output,
        "has_rerank": "has_rerank=" in metrics_output
    }
    
    for label, found in label_checks.items():
        status = "âœ…" if found else "âŒ"
        print(f"  {status} {label}")
    
    print("\nGrafana dashboard:")
    dashboard_path = project_root / "grafana/dashboards/rag.json"
    if dashboard_path.exists():
        import json
        try:
            with open(dashboard_path, 'r') as f:
                dashboard = json.load(f)
            
            panel_count = len(dashboard.get('panels', []))
            print(f"âœ… Dashboard with {panel_count} panels")
            
            # Check for P50/P95 queries
            dashboard_str = json.dumps(dashboard)
            has_percentiles = "histogram_quantile(0.50" in dashboard_str and "histogram_quantile(0.95" in dashboard_str
            print(f"âœ… P50/P95 latency queries: {has_percentiles}")
            
        except json.JSONDecodeError:
            print("âŒ Dashboard JSON is invalid")
    else:
        print("âŒ Grafana dashboard missing")
    
    print("\nðŸ§ª 5. Live Metrics Test")
    print("-" * 40)
    
    # Run a more comprehensive demo
    print("Running comprehensive metrics demo...")
    demo_result = demo_metrics()
    print(f"Demo completed: {demo_result}")
    
    print("\nðŸ“Š 6. Metrics Endpoints")
    print("-" * 40)
    
    print("Testing middleware integration...")
    try:
        demo_middleware()
        print("âœ… Middleware demo completed")
    except Exception as e:
        print(f"âš ï¸ Middleware demo issue: {e}")
    
    print("\nðŸŽ‰ 7. Summary")
    print("-" * 40)
    
    print("Issue #236 Implementation Status:")
    print("âœ… Prometheus metrics collection implemented")
    print("âœ… Counters: queries_total, answers_total, errors_total")
    print("âœ… Histograms: retrieval_latency_ms, answer_latency_ms, k_used")
    print("âœ… Labels: provider, lang, has_rerank")
    print("âœ… FastAPI middleware for automatic collection")
    print("âœ… Grafana dashboard with P50/P95 latencies")
    print("âœ… /metrics endpoint for Prometheus scraping")
    
    print("\nReady for deployment and monitoring!")

async def test_api_metrics():
    """Test API-level metrics collection."""
    print("\nðŸŒ API Metrics Test")
    print("-" * 40)
    
    try:
        # This would normally test against a running server
        print("Note: To test API metrics, run the FastAPI server:")
        print("  python services/api/main.py")
        print("Then visit:")
        print("  http://localhost:8000/metrics")
        print("  http://localhost:8000/api/ask/?q=test&provider=openai")
        
    except Exception as e:
        print(f"API test skipped: {e}")

def create_monitoring_setup_guide():
    """Create a setup guide for monitoring."""
    guide_content = """# RAG System Monitoring Setup Guide

## Issue #236: Observability Implementation

This guide explains how to set up monitoring for the RAG system with Prometheus and Grafana.

## Components

### 1. Metrics Collection (`services/obs/metrics.py`)
- Prometheus counters for queries, answers, errors
- Histograms for latency and document usage
- Automatic labeling with provider, language, reranking

### 2. API Middleware (`services/api/middleware/metrics.py`)
- Automatic metrics collection for all RAG endpoints
- Request timing and error tracking
- Configurable endpoint filtering

### 3. Grafana Dashboard (`grafana/dashboards/rag.json`)
- Real-time monitoring of RAG performance
- P50/P95/P99 latency percentiles
- Error rates and success rates
- Provider and language distribution

## Quick Start

### 1. Run the API Server
```bash
python services/api/main.py
```

### 2. Access Metrics
```bash
curl http://localhost:8000/metrics
```

### 3. Test with Queries
```bash
curl "http://localhost:8000/api/ask/?q=What%20is%20AI&provider=openai&k=5"
```

### 4. Import Grafana Dashboard
1. Open Grafana
2. Go to Dashboards > Import
3. Upload `grafana/dashboards/rag.json`
4. Configure Prometheus datasource

## Metrics Available

### Counters
- `rag_queries_total{provider,lang,has_rerank,status}` - Total queries processed
- `rag_answers_total{provider,lang,has_rerank}` - Total answers generated  
- `rag_errors_total{provider,error_type,component}` - Total errors by type

### Histograms
- `rag_retrieval_latency_milliseconds{provider,k_used}` - Document retrieval time
- `rag_answer_latency_milliseconds{provider,lang}` - Answer generation time
- `rag_k_documents_used{provider,query_type}` - Number of documents used

## Dashboard Features

- **Query Rate**: Requests per second by provider
- **Success Rate**: Percentage of successful queries
- **Latency Percentiles**: P50/P95/P99 for retrieval and answer generation
- **Error Analysis**: Error types and components
- **Resource Usage**: Document counts and distributions
- **Provider Breakdown**: Query distribution by provider and language

## Alerting Recommendations

### Critical Alerts
- Error rate > 5% for 5 minutes
- P95 answer latency > 10 seconds
- No queries received for 10 minutes

### Warning Alerts  
- P95 retrieval latency > 500ms
- Error rate > 1% for 10 minutes
- Query rate increase > 200% from baseline

## Troubleshooting

### Common Issues
1. **Metrics not appearing**: Check FastAPI middleware setup
2. **High latency**: Review provider configurations
3. **High error rate**: Check logs for specific error types

### Debug Commands
```bash
# Check metrics collection
python -c "from services.obs.metrics import demo_metrics; demo_metrics()"

# Test middleware
python -c "from services.api.middleware.metrics import demo_middleware; demo_middleware()"
```

## Production Considerations

1. **Retention**: Configure appropriate metrics retention (30-90 days)
2. **Sampling**: Consider sampling for high-volume deployments
3. **Security**: Secure /metrics endpoint in production
4. **Backup**: Regular backup of Grafana dashboards and configs

---

For more information, see Issue #236: Observability: Prometheus counters + logs
"""
    
    guide_path = Path(__file__).parent / "MONITORING_SETUP_GUIDE.md"
    with open(guide_path, 'w') as f:
        f.write(guide_content)
    
    print(f"ðŸ“– Monitoring setup guide created: {guide_path}")

if __name__ == "__main__":
    asyncio.run(demo_observability())
    asyncio.run(test_api_metrics())
    create_monitoring_setup_guide()
