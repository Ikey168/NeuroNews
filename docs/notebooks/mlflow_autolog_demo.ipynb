{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e84e0b9",
   "metadata": {},
   "source": [
    "# MLflow Autologging Demo with Scikit-Learn\n",
    "\n",
    "**Issue #219: Autologging demo (sklearn) + template notebook**\n",
    "\n",
    "This notebook demonstrates MLflow's autologging capabilities with scikit-learn models. MLflow autologging automatically captures:\n",
    "\n",
    "- **Parameters**: Model hyperparameters, preprocessing settings\n",
    "- **Metrics**: Training and validation metrics\n",
    "- **Artifacts**: Trained models, plots, feature importance\n",
    "- **Model Signatures**: Input/output schema for model serving\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand MLflow autologging setup and configuration\n",
    "2. Train multiple sklearn models with automatic experiment tracking\n",
    "3. Compare model performance using MLflow UI\n",
    "4. Load and use logged models for prediction\n",
    "5. Understand best practices for ML experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcfef25",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's import the required libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468560a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "# Data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Scikit-learn for machine learning\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    mean_squared_error, r2_score, mean_absolute_error\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6cb3fe",
   "metadata": {},
   "source": [
    "## 2. MLflow Configuration\n",
    "\n",
    "Let's configure MLflow tracking and enable autologging for scikit-learn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9683582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow configuration\n",
    "TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://localhost:5001\")\n",
    "EXPERIMENT_NAME = \"sklearn-autolog-demo-notebook\"\n",
    "\n",
    "# Set up MLflow tracking\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "\n",
    "try:\n",
    "    # Create or set experiment\n",
    "    experiment = mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "    print(f\"‚úÖ MLflow experiment set: {EXPERIMENT_NAME}\")\n",
    "    print(f\"üîó Tracking URI: {TRACKING_URI}\")\n",
    "    print(f\"üìä Experiment ID: {experiment.experiment_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå MLflow setup failed: {e}\")\n",
    "    print(\"üí° Make sure MLflow server is running: mlflow server --host 0.0.0.0 --port 5001\")\n",
    "\n",
    "# Enable sklearn autologging with comprehensive settings\n",
    "mlflow.sklearn.autolog(\n",
    "    log_input_examples=True,    # Log sample input data for model serving\n",
    "    log_model_signatures=True,  # Log input/output schema\n",
    "    log_models=True,           # Log trained models as artifacts\n",
    "    log_datasets=True,         # Log dataset information\n",
    "    disable=False,             # Enable autologging\n",
    "    exclusive=False,           # Allow manual logging too\n",
    "    disable_for_unsupported_versions=False,\n",
    "    silent=False               # Show autologging messages\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ MLflow sklearn autologging enabled!\")\n",
    "print(\"üìù The following will be automatically logged:\")\n",
    "print(\"   - Model parameters (hyperparameters)\")\n",
    "print(\"   - Training metrics\")\n",
    "print(\"   - Model artifacts\")\n",
    "print(\"   - Model signatures\")\n",
    "print(\"   - Input examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fa0ee4",
   "metadata": {},
   "source": [
    "## 3. Dataset Creation\n",
    "\n",
    "Let's create synthetic datasets for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5394930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_dataset(n_samples=1000, n_features=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a synthetic classification dataset.\n",
    "    \"\"\"\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=int(n_features * 0.7),\n",
    "        n_redundant=int(n_features * 0.2),\n",
    "        n_clusters_per_class=1,\n",
    "        class_sep=1.0,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Create feature names\n",
    "    feature_names = [f\"feature_{i+1}\" for i in range(n_features)]\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    X_df = pd.DataFrame(X, columns=feature_names)\n",
    "    y_series = pd.Series(y, name=\"target\")\n",
    "    \n",
    "    return X_df, y_series\n",
    "\n",
    "def create_regression_dataset(n_samples=1000, n_features=8, noise=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a synthetic regression dataset.\n",
    "    \"\"\"\n",
    "    X, y = make_regression(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=int(n_features * 0.8),\n",
    "        noise=noise,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Create feature names\n",
    "    feature_names = [f\"feature_{i+1}\" for i in range(n_features)]\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    X_df = pd.DataFrame(X, columns=feature_names)\n",
    "    y_series = pd.Series(y, name=\"target\")\n",
    "    \n",
    "    return X_df, y_series\n",
    "\n",
    "# Create datasets\n",
    "print(\"üìä Creating synthetic datasets...\")\n",
    "\n",
    "# Classification dataset\n",
    "X_clf, y_clf = create_classification_dataset(n_samples=1500, n_features=12, random_state=42)\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf\n",
    ")\n",
    "\n",
    "# Regression dataset\n",
    "X_reg, y_reg = create_regression_dataset(n_samples=1200, n_features=10, noise=0.15, random_state=42)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Classification dataset: {X_clf.shape[0]} samples, {X_clf.shape[1]} features\")\n",
    "print(f\"   - Train: {len(X_train_clf)} samples\")\n",
    "print(f\"   - Test: {len(X_test_clf)} samples\")\n",
    "print(f\"   - Classes: {sorted(y_clf.unique())}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Regression dataset: {X_reg.shape[0]} samples, {X_reg.shape[1]} features\")\n",
    "print(f\"   - Train: {len(X_train_reg)} samples\")\n",
    "print(f\"   - Test: {len(X_test_reg)} samples\")\n",
    "print(f\"   - Target range: [{y_reg.min():.2f}, {y_reg.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fec959",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis\n",
    "\n",
    "Let's visualize our datasets to understand their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ffd5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Classification target distribution\n",
    "y_clf.value_counts().plot(kind='bar', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Classification Target Distribution')\n",
    "axes[0, 0].set_xlabel('Class')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# Classification feature correlation heatmap (first 8 features)\n",
    "corr_clf = X_clf.iloc[:, :8].corr()\n",
    "sns.heatmap(corr_clf, annot=True, cmap='coolwarm', center=0, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Classification Features Correlation')\n",
    "\n",
    "# Regression target distribution\n",
    "y_reg.hist(bins=30, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Regression Target Distribution')\n",
    "axes[1, 0].set_xlabel('Target Value')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Regression feature correlation heatmap (first 8 features)\n",
    "corr_reg = X_reg.iloc[:, :8].corr()\n",
    "sns.heatmap(corr_reg, annot=True, cmap='coolwarm', center=0, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Regression Features Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Dataset visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b860e",
   "metadata": {},
   "source": [
    "## 5. Classification Models with MLflow Autologging\n",
    "\n",
    "Now let's train multiple classification models and see how MLflow automatically logs everything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67bab29",
   "metadata": {},
   "source": [
    "### 5.1 Logistic Regression with Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f8ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with preprocessing pipeline\n",
    "print(\"üöÄ Training Logistic Regression with Pipeline...\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"logistic_regression_{datetime.now().strftime('%Y%m%d_%H%M%S')}\") as run:\n",
    "    \n",
    "    # Add custom tags\n",
    "    mlflow.set_tag(\"model_type\", \"logistic_regression\")\n",
    "    mlflow.set_tag(\"task_type\", \"classification\")\n",
    "    mlflow.set_tag(\"notebook_section\", \"5.1\")\n",
    "    mlflow.set_tag(\"dataset_type\", \"synthetic\")\n",
    "    \n",
    "    # Create pipeline (autologging will capture all parameters)\n",
    "    lr_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(\n",
    "            random_state=42,\n",
    "            max_iter=1000,\n",
    "            C=1.0,\n",
    "            solver='liblinear'\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train model (autologging captures training metrics)\n",
    "    lr_pipeline.fit(X_train_clf, y_train_clf)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_lr = lr_pipeline.predict(X_test_clf)\n",
    "    y_pred_proba_lr = lr_pipeline.predict_proba(X_test_clf)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    accuracy_lr = accuracy_score(y_test_clf, y_pred_lr)\n",
    "    \n",
    "    # Log custom metrics\n",
    "    mlflow.log_metric(\"test_accuracy\", accuracy_lr)\n",
    "    mlflow.log_metric(\"n_features\", X_train_clf.shape[1])\n",
    "    mlflow.log_metric(\"train_samples\", len(X_train_clf))\n",
    "    mlflow.log_metric(\"test_samples\", len(X_test_clf))\n",
    "    \n",
    "    # Log classification report as artifact\n",
    "    report_lr = classification_report(y_test_clf, y_pred_lr, output_dict=True)\n",
    "    mlflow.log_dict(report_lr, \"classification_report.json\")\n",
    "    \n",
    "    # Create and log confusion matrix plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test_clf, y_pred_lr)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Logistic Regression - Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    mlflow.log_figure(plt.gcf(), \"confusion_matrix.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Logistic Regression completed!\")\n",
    "    print(f\"   - Test Accuracy: {accuracy_lr:.4f}\")\n",
    "    print(f\"   - MLflow Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    lr_run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fbcc1f",
   "metadata": {},
   "source": [
    "### 5.2 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "print(\"üå≤ Training Random Forest Classifier...\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"random_forest_{datetime.now().strftime('%Y%m%d_%H%M%S')}\") as run:\n",
    "    \n",
    "    # Add custom tags\n",
    "    mlflow.set_tag(\"model_type\", \"random_forest\")\n",
    "    mlflow.set_tag(\"task_type\", \"classification\")\n",
    "    mlflow.set_tag(\"notebook_section\", \"5.2\")\n",
    "    mlflow.set_tag(\"dataset_type\", \"synthetic\")\n",
    "    \n",
    "    # Create model (autologging will capture all hyperparameters)\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        bootstrap=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    rf_model.fit(X_train_clf, y_train_clf)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_rf = rf_model.predict(X_test_clf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_rf = accuracy_score(y_test_clf, y_pred_rf)\n",
    "    \n",
    "    # Log additional metrics\n",
    "    mlflow.log_metric(\"test_accuracy\", accuracy_rf)\n",
    "    mlflow.log_metric(\"feature_importances_mean\", np.mean(rf_model.feature_importances_))\n",
    "    mlflow.log_metric(\"feature_importances_std\", np.std(rf_model.feature_importances_))\n",
    "    \n",
    "    # Log feature importances\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': X_train_clf.columns,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    mlflow.log_dict(feature_importance_df.to_dict('records'), \"feature_importances.json\")\n",
    "    \n",
    "    # Create and log feature importance plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_importance_df.head(10), x='importance', y='feature')\n",
    "    plt.title('Random Forest - Top 10 Feature Importances')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    mlflow.log_figure(plt.gcf(), \"feature_importances.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Random Forest completed!\")\n",
    "    print(f\"   - Test Accuracy: {accuracy_rf:.4f}\")\n",
    "    print(f\"   - Top feature: {feature_importance_df.iloc[0]['feature']} ({feature_importance_df.iloc[0]['importance']:.4f})\")\n",
    "    print(f\"   - MLflow Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    rf_run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e904c202",
   "metadata": {},
   "source": [
    "### 5.3 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e14baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine with different parameters\n",
    "print(\"‚ö° Training Support Vector Machine...\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"svm_{datetime.now().strftime('%Y%m%d_%H%M%S')}\") as run:\n",
    "    \n",
    "    # Add custom tags\n",
    "    mlflow.set_tag(\"model_type\", \"svm\")\n",
    "    mlflow.set_tag(\"task_type\", \"classification\")\n",
    "    mlflow.set_tag(\"notebook_section\", \"5.3\")\n",
    "    mlflow.set_tag(\"dataset_type\", \"synthetic\")\n",
    "    \n",
    "    # Create pipeline with SVM\n",
    "    svm_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', SVC(\n",
    "            random_state=42,\n",
    "            C=1.0,\n",
    "            kernel='rbf',\n",
    "            gamma='scale',\n",
    "            probability=True  # Enable probability predictions\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    svm_pipeline.fit(X_train_clf, y_train_clf)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_svm = svm_pipeline.predict(X_test_clf)\n",
    "    y_pred_proba_svm = svm_pipeline.predict_proba(X_test_clf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_svm = accuracy_score(y_test_clf, y_pred_svm)\n",
    "    \n",
    "    # Log custom metrics\n",
    "    mlflow.log_metric(\"test_accuracy\", accuracy_svm)\n",
    "    mlflow.log_metric(\"n_support_vectors\", sum(svm_pipeline.named_steps['classifier'].n_support_))\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(svm_pipeline, X_train_clf, y_train_clf, cv=5)\n",
    "    mlflow.log_metric(\"cv_score_mean\", cv_scores.mean())\n",
    "    mlflow.log_metric(\"cv_score_std\", cv_scores.std())\n",
    "    \n",
    "    print(f\"‚úÖ SVM completed!\")\n",
    "    print(f\"   - Test Accuracy: {accuracy_svm:.4f}\")\n",
    "    print(f\"   - CV Score: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "    print(f\"   - Support Vectors: {sum(svm_pipeline.named_steps['classifier'].n_support_)}\")\n",
    "    print(f\"   - MLflow Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    svm_run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da21292d",
   "metadata": {},
   "source": [
    "## 6. Regression Models with MLflow Autologging\n",
    "\n",
    "Now let's demonstrate autologging with regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdbe7b1",
   "metadata": {},
   "source": [
    "### 6.1 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59805372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression with Pipeline\n",
    "print(\"üìà Training Linear Regression...\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"linear_regression_{datetime.now().strftime('%Y%m%d_%H%M%S')}\") as run:\n",
    "    \n",
    "    # Add custom tags\n",
    "    mlflow.set_tag(\"model_type\", \"linear_regression\")\n",
    "    mlflow.set_tag(\"task_type\", \"regression\")\n",
    "    mlflow.set_tag(\"notebook_section\", \"6.1\")\n",
    "    mlflow.set_tag(\"dataset_type\", \"synthetic\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    linear_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    linear_pipeline.fit(X_train_reg, y_train_reg)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_linear = linear_pipeline.predict(X_test_reg)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse_linear = mean_squared_error(y_test_reg, y_pred_linear)\n",
    "    r2_linear = r2_score(y_test_reg, y_pred_linear)\n",
    "    mae_linear = mean_absolute_error(y_test_reg, y_pred_linear)\n",
    "    \n",
    "    # Log additional metrics\n",
    "    mlflow.log_metric(\"test_mse\", mse_linear)\n",
    "    mlflow.log_metric(\"test_r2\", r2_linear)\n",
    "    mlflow.log_metric(\"test_mae\", mae_linear)\n",
    "    mlflow.log_metric(\"test_rmse\", np.sqrt(mse_linear))\n",
    "    \n",
    "    # Create and log prediction vs actual plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test_reg, y_pred_linear, alpha=0.6)\n",
    "    plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'Linear Regression - Predicted vs Actual (R¬≤ = {r2_linear:.4f})')\n",
    "    plt.tight_layout()\n",
    "    mlflow.log_figure(plt.gcf(), \"predictions_vs_actual.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Linear Regression completed!\")\n",
    "    print(f\"   - Test R¬≤: {r2_linear:.4f}\")\n",
    "    print(f\"   - Test RMSE: {np.sqrt(mse_linear):.4f}\")\n",
    "    print(f\"   - Test MAE: {mae_linear:.4f}\")\n",
    "    print(f\"   - MLflow Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    linear_run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd151532",
   "metadata": {},
   "source": [
    "### 6.2 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2538ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor\n",
    "print(\"üå≤ Training Random Forest Regressor...\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"rf_regressor_{datetime.now().strftime('%Y%m%d_%H%M%S')}\") as run:\n",
    "    \n",
    "    # Add custom tags\n",
    "    mlflow.set_tag(\"model_type\", \"random_forest_regressor\")\n",
    "    mlflow.set_tag(\"task_type\", \"regression\")\n",
    "    mlflow.set_tag(\"notebook_section\", \"6.2\")\n",
    "    mlflow.set_tag(\"dataset_type\", \"synthetic\")\n",
    "    \n",
    "    # Create model\n",
    "    rf_regressor = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        bootstrap=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    rf_regressor.fit(X_train_reg, y_train_reg)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_rf_reg = rf_regressor.predict(X_test_reg)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse_rf_reg = mean_squared_error(y_test_reg, y_pred_rf_reg)\n",
    "    r2_rf_reg = r2_score(y_test_reg, y_pred_rf_reg)\n",
    "    mae_rf_reg = mean_absolute_error(y_test_reg, y_pred_rf_reg)\n",
    "    \n",
    "    # Log additional metrics\n",
    "    mlflow.log_metric(\"test_mse\", mse_rf_reg)\n",
    "    mlflow.log_metric(\"test_r2\", r2_rf_reg)\n",
    "    mlflow.log_metric(\"test_mae\", mae_rf_reg)\n",
    "    mlflow.log_metric(\"test_rmse\", np.sqrt(mse_rf_reg))\n",
    "    \n",
    "    # Log feature importances\n",
    "    feature_importance_reg_df = pd.DataFrame({\n",
    "        'feature': X_train_reg.columns,\n",
    "        'importance': rf_regressor.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    mlflow.log_dict(feature_importance_reg_df.to_dict('records'), \"feature_importances.json\")\n",
    "    \n",
    "    # Create residuals plot\n",
    "    residuals = y_test_reg - y_pred_rf_reg\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Residuals vs predicted\n",
    "    axes[0].scatter(y_pred_rf_reg, residuals, alpha=0.6)\n",
    "    axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0].set_xlabel('Predicted Values')\n",
    "    axes[0].set_ylabel('Residuals')\n",
    "    axes[0].set_title('Residuals vs Predicted')\n",
    "    \n",
    "    # Feature importances\n",
    "    sns.barplot(data=feature_importance_reg_df.head(8), x='importance', y='feature', ax=axes[1])\n",
    "    axes[1].set_title('Top 8 Feature Importances')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    mlflow.log_figure(plt.gcf(), \"regression_analysis.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Random Forest Regressor completed!\")\n",
    "    print(f\"   - Test R¬≤: {r2_rf_reg:.4f}\")\n",
    "    print(f\"   - Test RMSE: {np.sqrt(mse_rf_reg):.4f}\")\n",
    "    print(f\"   - Top feature: {feature_importance_reg_df.iloc[0]['feature']} ({feature_importance_reg_df.iloc[0]['importance']:.4f})\")\n",
    "    print(f\"   - MLflow Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    rf_reg_run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d78661",
   "metadata": {},
   "source": [
    "## 7. Model Comparison and Analysis\n",
    "\n",
    "Let's compare all our trained models using MLflow data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c414ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get experiment and runs\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "print(f\"üìä Experiment Summary: {EXPERIMENT_NAME}\")\n",
    "print(f\"Total runs: {len(runs)}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Filter and display classification models\n",
    "classification_runs = runs[runs['tags.task_type'] == 'classification'].copy()\n",
    "if not classification_runs.empty:\n",
    "    print(\"üéØ CLASSIFICATION MODELS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for _, run in classification_runs.iterrows():\n",
    "        model_type = run.get('tags.model_type', 'Unknown')\n",
    "        accuracy = run.get('metrics.test_accuracy', 'N/A')\n",
    "        run_id = run['run_id'][:8]\n",
    "        \n",
    "        print(f\"  {model_type:20} | Accuracy: {accuracy:6.4f} | Run: {run_id}\")\n",
    "\n",
    "# Filter and display regression models\n",
    "regression_runs = runs[runs['tags.task_type'] == 'regression'].copy()\n",
    "if not regression_runs.empty:\n",
    "    print(\"\\nüìà REGRESSION MODELS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for _, run in regression_runs.iterrows():\n",
    "        model_type = run.get('tags.model_type', 'Unknown')\n",
    "        r2 = run.get('metrics.test_r2', 'N/A')\n",
    "        rmse = run.get('metrics.test_rmse', 'N/A')\n",
    "        run_id = run['run_id'][:8]\n",
    "        \n",
    "        print(f\"  {model_type:20} | R¬≤: {r2:6.4f} | RMSE: {rmse:7.4f} | Run: {run_id}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Create comparison visualization\n",
    "if not classification_runs.empty:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Classification accuracy comparison\n",
    "    plt.subplot(1, 2, 1)\n",
    "    if 'metrics.test_accuracy' in classification_runs.columns:\n",
    "        model_names = classification_runs['tags.model_type'].values\n",
    "        accuracies = classification_runs['metrics.test_accuracy'].values\n",
    "        \n",
    "        bars = plt.bar(model_names, accuracies)\n",
    "        plt.title('Classification Model Accuracy Comparison')\n",
    "        plt.ylabel('Test Accuracy')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{acc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Regression R¬≤ comparison\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if not regression_runs.empty and 'metrics.test_r2' in regression_runs.columns:\n",
    "        reg_model_names = regression_runs['tags.model_type'].values\n",
    "        r2_scores = regression_runs['metrics.test_r2'].values\n",
    "        \n",
    "        bars = plt.bar(reg_model_names, r2_scores)\n",
    "        plt.title('Regression Model R¬≤ Comparison')\n",
    "        plt.ylabel('Test R¬≤')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, r2 in zip(bars, r2_scores):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{r2:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nüîó View detailed results in MLflow UI: {TRACKING_URI}\")\n",
    "print(f\"üìÅ Experiment: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515c2ea",
   "metadata": {},
   "source": [
    "## 8. Loading and Using Logged Models\n",
    "\n",
    "One of the key benefits of MLflow is the ability to easily load and use previously trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b5e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best classification model (highest accuracy)\n",
    "if not classification_runs.empty and 'metrics.test_accuracy' in classification_runs.columns:\n",
    "    best_clf_run = classification_runs.loc[classification_runs['metrics.test_accuracy'].idxmax()]\n",
    "    best_clf_run_id = best_clf_run['run_id']\n",
    "    best_clf_model_type = best_clf_run['tags.model_type']\n",
    "    best_clf_accuracy = best_clf_run['metrics.test_accuracy']\n",
    "    \n",
    "    print(f\"üèÜ Loading best classification model: {best_clf_model_type}\")\n",
    "    print(f\"   Run ID: {best_clf_run_id}\")\n",
    "    print(f\"   Accuracy: {best_clf_accuracy:.4f}\")\n",
    "    \n",
    "    # Load model using MLflow\n",
    "    model_uri = f\"runs:/{best_clf_run_id}/model\"\n",
    "    loaded_clf_model = mlflow.sklearn.load_model(model_uri)\n",
    "    \n",
    "    # Test the loaded model\n",
    "    test_predictions = loaded_clf_model.predict(X_test_clf.head(5))\n",
    "    test_probabilities = loaded_clf_model.predict_proba(X_test_clf.head(5))\n",
    "    \n",
    "    print(\"\\nüß™ Testing loaded model on first 5 test samples:\")\n",
    "    for i in range(5):\n",
    "        actual = y_test_clf.iloc[i]\n",
    "        predicted = test_predictions[i]\n",
    "        prob = test_probabilities[i].max()\n",
    "        print(f\"   Sample {i+1}: Actual={actual}, Predicted={predicted}, Confidence={prob:.3f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Model loaded and tested successfully!\")\n",
    "\n",
    "# Load the best regression model (highest R¬≤)\n",
    "if not regression_runs.empty and 'metrics.test_r2' in regression_runs.columns:\n",
    "    best_reg_run = regression_runs.loc[regression_runs['metrics.test_r2'].idxmax()]\n",
    "    best_reg_run_id = best_reg_run['run_id']\n",
    "    best_reg_model_type = best_reg_run['tags.model_type']\n",
    "    best_reg_r2 = best_reg_run['metrics.test_r2']\n",
    "    \n",
    "    print(f\"\\nüèÜ Loading best regression model: {best_reg_model_type}\")\n",
    "    print(f\"   Run ID: {best_reg_run_id}\")\n",
    "    print(f\"   R¬≤: {best_reg_r2:.4f}\")\n",
    "    \n",
    "    # Load model\n",
    "    model_uri = f\"runs:/{best_reg_run_id}/model\"\n",
    "    loaded_reg_model = mlflow.sklearn.load_model(model_uri)\n",
    "    \n",
    "    # Test the loaded model\n",
    "    test_reg_predictions = loaded_reg_model.predict(X_test_reg.head(5))\n",
    "    \n",
    "    print(\"\\nüß™ Testing loaded regression model on first 5 test samples:\")\n",
    "    for i in range(5):\n",
    "        actual = y_test_reg.iloc[i]\n",
    "        predicted = test_reg_predictions[i]\n",
    "        error = abs(actual - predicted)\n",
    "        print(f\"   Sample {i+1}: Actual={actual:8.3f}, Predicted={predicted:8.3f}, Error={error:6.3f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Regression model loaded and tested successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91031ea",
   "metadata": {},
   "source": [
    "## 9. Best Practices and Key Takeaways\n",
    "\n",
    "Let's summarize what we've learned about MLflow autologging with scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b64e9",
   "metadata": {},
   "source": [
    "### 9.1 What MLflow Autologging Captured\n",
    "\n",
    "MLflow autologging automatically captured:\n",
    "\n",
    "**Parameters:**\n",
    "- All model hyperparameters (C, max_iter, n_estimators, etc.)\n",
    "- Pipeline step parameters\n",
    "- Preprocessing parameters\n",
    "\n",
    "**Metrics:**\n",
    "- Training score\n",
    "- Cross-validation scores (when applicable)\n",
    "- Custom metrics we logged manually\n",
    "\n",
    "**Artifacts:**\n",
    "- Trained model (serialized for reuse)\n",
    "- Model signature (input/output schema)\n",
    "- Input examples (for model serving)\n",
    "- Custom artifacts (plots, reports, etc.)\n",
    "\n",
    "**Metadata:**\n",
    "- Model type and framework version\n",
    "- Training duration\n",
    "- Custom tags we added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a75fe0",
   "metadata": {},
   "source": [
    "### 9.2 Best Practices Demonstrated\n",
    "\n",
    "1. **Consistent Tagging**: Use tags to categorize and filter runs\n",
    "2. **Custom Metrics**: Log additional metrics beyond autologged ones\n",
    "3. **Visualization**: Save plots as artifacts for later analysis\n",
    "4. **Model Comparison**: Use MLflow data for systematic comparison\n",
    "5. **Model Reuse**: Load and test saved models easily\n",
    "6. **Documentation**: Use meaningful run names and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b546a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"üéâ MLflow Autologging Demo Complete!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_runs = len(runs) if 'runs' in locals() else 0\n",
    "print(f\"üìä Total ML experiments tracked: {total_runs}\")\n",
    "print(f\"üîó MLflow UI: {TRACKING_URI}\")\n",
    "print(f\"üìÅ Experiment: {EXPERIMENT_NAME}\")\n",
    "\n",
    "print(\"\\n‚ú® What you've learned:\")\n",
    "print(\"   ‚úÖ MLflow autologging setup and configuration\")\n",
    "print(\"   ‚úÖ Automatic parameter and metric tracking\")\n",
    "print(\"   ‚úÖ Model artifact management\")\n",
    "print(\"   ‚úÖ Custom logging and visualization\")\n",
    "print(\"   ‚úÖ Model loading and reuse\")\n",
    "print(\"   ‚úÖ Experiment comparison and analysis\")\n",
    "\n",
    "print(\"\\nüöÄ Next steps:\")\n",
    "print(\"   ‚Ä¢ Explore the MLflow UI to see all logged information\")\n",
    "print(\"   ‚Ä¢ Try modifying hyperparameters and compare results\")\n",
    "print(\"   ‚Ä¢ Register your best models for production use\")\n",
    "print(\"   ‚Ä¢ Set up model serving with MLflow Models\")\n",
    "\n",
    "print(\"\\nüí° Remember: MLflow autologging works with minimal code changes\")\n",
    "print(\"   Just call mlflow.sklearn.autolog() and start training!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
