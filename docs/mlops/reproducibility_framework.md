# MLflow Reproducibility Framework

**Issue #222**: Environment capture and dataset manifest generation for experiment reproducibility.

This document describes the reproducibility framework implemented to ensure MLflow experiments can be perfectly reproduced by capturing environment state and data integrity information.

## Overview

The reproducibility framework provides two main components:

1. **Environment Capture** (`scripts/capture_env.py`): Captures the complete environment state including dependencies, system information, and hardware configuration
2. **Data Manifest Generation** (`services/mlops/data_manifest.py`): Creates manifests with file hashes and metadata to ensure data integrity

## Features

### Environment Capture

- **Python Dependencies**: Complete pip freeze with exact versions
- **Conda Packages**: Conda environment export (if available)
- **System Information**: OS, CPU, memory, GPU details
- **Python Environment**: Version, implementation, executable paths
- **Git Information**: Commit hash, branch, uncommitted changes
- **Environment Variables**: Relevant variables (CUDA, MLFLOW, etc.)

### Data Manifest Generation

- **File Hashing**: MD5, SHA1, SHA256, SHA512 support
- **Metadata Collection**: File sizes, modification times, permissions
- **Directory Scanning**: Recursive processing with include/exclude patterns
- **Performance Monitoring**: Processing time and throughput metrics
- **Validation**: Integrity checking against existing manifests

## Usage

### Environment Capture

#### Standalone Usage

```bash
# Basic usage
python scripts/capture_env.py

# With custom output directory
python scripts/capture_env.py --output-dir ./my_env_artifacts

# Log to active MLflow run
python scripts/capture_env.py --mlflow-run
```

#### Programmatic Usage

```python
from scripts.capture_env import capture_environment

# Capture environment
artifacts = capture_environment(
    output_dir="./env_artifacts",
    log_to_mlflow=True
)

# artifacts contains paths to generated files:
# - requirements.txt: pip freeze output
# - system_info.json: comprehensive system information
# - conda_packages.txt: conda packages (if available)
```

### Data Manifest Generation

#### Standalone Usage

```bash
# Generate manifest for directory
python -m services.mlops.data_manifest /path/to/data

# With custom output
python -m services.mlops.data_manifest /path/to/data --output manifest.json

# Include/exclude patterns
python -m services.mlops.data_manifest /path/to/data \
    --include "*.csv" "*.json" \
    --exclude "*.tmp" "*.log"

# Validate against existing manifest
python -m services.mlops.data_manifest /path/to/data \
    --validate existing_manifest.json
```

#### Programmatic Usage

```python
from services.mlops.data_manifest import DataManifestGenerator

# Create generator
generator = DataManifestGenerator(hash_algorithms=['md5', 'sha256'])

# Generate manifest
manifest = generator.generate_manifest(
    "/path/to/data",
    include_patterns=["*.csv", "*.json"],
    exclude_patterns=["*.tmp", "*.log"],
    metadata={"experiment": "my_experiment"}
)

# Save manifest
generator.save_manifest(manifest, "data_manifest.json")

# Log to MLflow
generator.log_to_mlflow(manifest)

# Validate data integrity
validation_results = generator.validate_manifest(manifest, "/path/to/data")
```

## Integration with Existing Components

### RAG Indexer Integration

The RAG indexer (`jobs/rag/indexer.py`) automatically captures reproducibility information:

```python
from jobs.rag.indexer import RAGIndexer

# Create indexer with reproducibility features enabled
indexer = RAGIndexer(
    capture_environment=True,     # Capture environment info
    capture_data_manifest=True    # Generate data manifest
)

# Run indexing (automatically captures reproducibility info)
results = await indexer.index_documents(documents)
```

### Training Script Integration

Training scripts can easily integrate reproducibility features:

```python
import mlflow
from scripts.capture_env import capture_environment
from services.mlops.data_manifest import DataManifestGenerator

with mlflow.start_run() as run:
    # Capture environment
    capture_environment(log_to_mlflow=True)
    
    # Generate data manifest
    generator = DataManifestGenerator()
    manifest = generator.generate_manifest("/path/to/training/data")
    generator.log_to_mlflow(manifest)
    
    # ... rest of training code
```

## Artifacts Generated

### Environment Artifacts

1. **requirements.txt**
   ```
   # Generated by capture_env.py on 2024-01-15T10:30:00
   # Python 3.9.7
   
   numpy==1.21.0
   pandas==1.3.3
   scikit-learn==1.0.2
   mlflow==2.8.0
   ...
   ```

2. **system_info.json**
   ```json
   {
     "timestamp": "2024-01-15T10:30:00",
     "platform": {
       "system": "Linux",
       "release": "5.4.0-74-generic",
       "machine": "x86_64"
     },
     "python": {
       "version": "3.9.7",
       "implementation": "CPython"
     },
     "cpu": {
       "cpu_count": 8,
       "model_name": "Intel(R) Core(TM) i7-8700K"
     },
     "gpu": {
       "cuda_available": true,
       "gpu_count": 1,
       "gpus": [...]
     },
     "git": {
       "commit_sha": "abc123...",
       "branch": "main",
       "has_uncommitted_changes": false
     }
   }
   ```

3. **conda_packages.txt** (if conda is available)
   ```
   # Generated by capture_env.py on 2024-01-15T10:30:00
   # Conda packages
   
   numpy=1.21.0=py39h20f2e39_0
   pandas=1.3.3=py39h8c16a72_0
   ...
   ```

### Data Manifest Artifacts

1. **data_manifest.json**
   ```json
   {
     "manifest_version": "1.0",
     "generated_at": "2024-01-15T10:30:00",
     "data_path": "/path/to/data",
     "hash_algorithms": ["md5", "sha256"],
     "files": [
       {
         "path": "/path/to/data/file1.csv",
         "relative_path": "file1.csv",
         "size_bytes": 1024,
         "size_human": "1.0 KB",
         "modified_time": "2024-01-15T09:00:00",
         "hashes": {
           "md5": "d41d8cd98f00b204e9800998ecf8427e",
           "sha256": "e3b0c44298fc1c149afbf4c8996fb924..."
         }
       }
     ],
     "summary": {
       "total_files": 10,
       "total_size_bytes": 10240,
       "total_size_human": "10.0 KB",
       "processing_time_seconds": 0.5
     }
   }
   ```

## MLflow Integration

### Automatic Logging

When `log_to_mlflow=True`, the following information is automatically logged:

**Environment Capture**:
- Artifacts: `requirements.txt`, `system_info.json`, `conda_packages.txt`
- Parameters: `python_version`, `platform_system`, `cpu_count`, `gpu_count`
- Parameters: `git_commit`, `git_branch`, `git_uncommitted_changes`

**Data Manifest**:
- Artifacts: `data_manifest.json`
- Metrics: `data_files_count`, `data_size_bytes`, `data_processing_time`
- Parameters: `data_path`, `hash_algorithms`, `manifest_version`

### Viewing in MLflow UI

1. **Start MLflow UI**:
   ```bash
   mlflow ui --backend-store-uri file:./mlruns
   ```

2. **Navigate to Experiments**: View experiments in the web interface

3. **Check Artifacts**: 
   - Environment artifacts under `environment/`
   - Data manifests under `data_manifest/` or custom path

4. **Compare Runs**: Use MLflow's comparison features to analyze differences

## Validation and Reproducibility

### Validating Environment

```python
# Load system info from MLflow artifact
import json
with open("system_info.json") as f:
    original_env = json.load(f)

# Compare with current environment
current_artifacts = capture_environment(log_to_mlflow=False)
with open(current_artifacts["system_info.json"]) as f:
    current_env = json.load(f)

# Check key differences
if original_env["python"]["version"] != current_env["python"]["version"]:
    print("⚠️ Python version mismatch!")
```

### Validating Data Integrity

```python
# Load original manifest
with open("data_manifest.json") as f:
    original_manifest = json.load(f)

# Validate current data
generator = DataManifestGenerator()
validation_results = generator.validate_manifest(
    original_manifest, 
    "/path/to/data"
)

if not validation_results["valid"]:
    print("❌ Data validation failed:")
    print(f"Missing files: {validation_results['missing_files']}")
    print(f"Hash mismatches: {validation_results['hash_mismatches']}")
```

## Best Practices

### 1. Always Capture Environment

```python
# In training/inference scripts
def run_experiment():
    with mlflow.start_run():
        # First thing: capture environment
        capture_environment(log_to_mlflow=True)
        
        # Then run your experiment
        # ...
```

### 2. Manifest Critical Data

```python
# Generate manifests for training data, not temporary files
generator = DataManifestGenerator()

# Include only relevant files
manifest = generator.generate_manifest(
    data_path,
    include_patterns=["*.csv", "*.json", "*.parquet"],
    exclude_patterns=["*.tmp", "*.log", "*cache*"]
)
```

### 3. Version Control Integration

```python
# Check for uncommitted changes before important runs
import subprocess

def check_git_status():
    result = subprocess.run(
        ["git", "status", "--porcelain"], 
        capture_output=True, text=True
    )
    
    if result.stdout.strip():
        print("⚠️ Warning: Uncommitted changes detected!")
        print("Consider committing changes for full reproducibility")
```

### 4. Validate Before Important Runs

```python
# Before production runs, validate data hasn't changed
def validate_data_integrity(manifest_path, data_path):
    with open(manifest_path) as f:
        manifest = json.load(f)
    
    generator = DataManifestGenerator()
    results = generator.validate_manifest(manifest, data_path)
    
    if not results["valid"]:
        raise ValueError("Data integrity check failed!")
```

## Demo Script

Run the complete demo to see all features:

```bash
python demo_reproducibility_features.py
```

This demonstrates:
- Environment capture
- Data manifest generation
- Manifest validation
- MLflow integration
- Error detection

## Troubleshooting

### Common Issues

1. **Import Errors**
   ```
   ImportError: No module named 'services.mlops.data_manifest'
   ```
   **Solution**: Ensure you're running from the project root directory

2. **Permission Errors**
   ```
   PermissionError: [Errno 13] Permission denied: '/tmp/...'
   ```
   **Solution**: Use a writable output directory or adjust permissions

3. **Git Not Found**
   ```
   FileNotFoundError: [Errno 2] No such file or directory: 'git'
   ```
   **Solution**: Install git or the capture will skip git information

4. **CUDA Not Available**
   ```
   Warning: Failed to get GPU info: No module named 'torch'
   ```
   **Solution**: Install PyTorch for GPU detection or ignore if not needed

### Performance Considerations

- **Large Directories**: Use `max_files` parameter to limit processing
- **Network Storage**: Local processing is faster than network drives
- **Hash Algorithms**: MD5 is fastest, SHA256 is more secure
- **Exclude Patterns**: Skip unnecessary files (logs, caches, etc.)

## Future Enhancements

1. **Automatic Dependency Analysis**: Detect actual dependencies used
2. **Container Image Capture**: Docker/Singularity image information
3. **Hardware Benchmarking**: Standardized performance metrics
4. **Cloud Environment Detection**: AWS/GCP/Azure instance metadata
5. **Differential Manifests**: Only track changes between runs
