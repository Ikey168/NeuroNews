---
# Prometheus monitoring rules for node autoscaling and bin-packing
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: neuronews-autoscaling-monitoring
  namespace: monitoring
  labels:
    app: prometheus
    component: autoscaling
    purpose: cost-optimization
spec:
  groups:
  - name: neuronews.autoscaling.cluster
    interval: 30s
    rules:
    # Cluster Autoscaler Health
    - alert: ClusterAutoscalerDown
      expr: up{job="cluster-autoscaler"} == 0
      for: 5m
      labels:
        severity: critical
        component: cluster-autoscaler
      annotations:
        summary: "Cluster Autoscaler is down"
        description: "Cluster Autoscaler has been down for more than 5 minutes"
        runbook_url: "https://docs.aws.amazon.com/eks/latest/best-practices/cas.html"

    - alert: ClusterAutoscalerHighErrorRate
      expr: rate(cluster_autoscaler_errors_total[5m]) > 0.1
      for: 5m
      labels:
        severity: warning
        component: cluster-autoscaler
      annotations:
        summary: "High error rate in Cluster Autoscaler"
        description: "Cluster Autoscaler error rate is {{ $value }} errors/second"

    # Node Scaling Metrics
    - record: neuronews:node_count:by_lifecycle
      expr: count by (node_lifecycle) (kube_node_labels{label_node_lifecycle!=""})

    - record: neuronews:node_utilization:cpu
      expr: (1 - rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100

    - record: neuronews:node_utilization:memory
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100

    # Scale Down Opportunities
    - alert: NodeUnderutilized
      expr: |
        (
          neuronews:node_utilization:cpu < 20 and
          neuronews:node_utilization:memory < 30
        ) and on(instance) kube_node_labels{label_node_lifecycle="on-demand"}
      for: 15m
      labels:
        severity: info
        component: cost-optimization
      annotations:
        summary: "Node {{ $labels.instance }} is underutilized"
        description: "Node has CPU < 20% and memory < 30% for 15+ minutes"

    # Spot Instance Monitoring
    - record: neuronews:spot_instance:interruption_rate
      expr: rate(kube_node_status_condition{condition="Ready",status="false"}[1h]) and on(instance) kube_node_labels{label_node_lifecycle="spot"}

    - alert: HighSpotInterruptionRate
      expr: neuronews:spot_instance:interruption_rate > 0.1
      for: 5m
      labels:
        severity: warning
        component: spot-instances
      annotations:
        summary: "High spot instance interruption rate"
        description: "Spot instance interruption rate is {{ $value }} per hour"

  - name: neuronews.autoscaling.workload
    interval: 30s
    rules:
    # Pod Distribution Metrics
    - record: neuronews:pod_distribution:by_node_type
      expr: count by (node_lifecycle, pipeline) (kube_pod_info * on(node) group_left(label_node_lifecycle) kube_node_labels{label_node_lifecycle!=""})

    - record: neuronews:pod_distribution:by_priority
      expr: count by (priority_class, pipeline) (kube_pod_info{created_by_kind!="Job"})

    # Resource Efficiency Metrics
    - record: neuronews:resource_efficiency:cpu
      expr: |
        (
          rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[5m]) /
          (kube_pod_container_resource_requests{resource="cpu"} > 0)
        ) * 100

    - record: neuronews:resource_efficiency:memory
      expr: |
        (
          container_memory_working_set_bytes{container!="POD",container!=""} /
          (kube_pod_container_resource_requests{resource="memory"} > 0)
        ) * 100

    # Bin-packing Effectiveness
    - record: neuronews:binpacking:node_density
      expr: count by (node) (kube_pod_info{created_by_kind!="Job"})

    - alert: PoorBinPacking
      expr: neuronews:binpacking:node_density < 3 and on(node) kube_node_labels{label_node_lifecycle="on-demand"}
      for: 10m
      labels:
        severity: info
        component: bin-packing
      annotations:
        summary: "Poor bin-packing on node {{ $labels.node }}"
        description: "Node has less than 3 pods, may indicate poor resource utilization"

    # Priority Class Distribution
    - alert: LowPriorityPodsOnExpensiveNodes
      expr: |
        (
          count by (node) (
            kube_pod_info{priority_class=~"neuronews-batch|neuronews-dev|neuronews-spot"} 
            * on(node) group_left(label_node_lifecycle) 
            kube_node_labels{label_node_lifecycle="on-demand"}
          )
        ) > 2
      for: 5m
      labels:
        severity: info
        component: cost-optimization
      annotations:
        summary: "Low priority pods on expensive on-demand nodes"
        description: "Node {{ $labels.node }} has {{ $value }} low-priority pods on on-demand instance"

  - name: neuronews.autoscaling.cost
    interval: 1m
    rules:
    # Cost Tracking Metrics
    - record: neuronews:cost:hourly_node_cost
      expr: |
        (
          kube_node_labels{label_node_lifecycle="on-demand"} * 0.096 +  # m5.large on-demand
          kube_node_labels{label_node_lifecycle="spot"} * 0.029         # m5.large spot
        )

    - record: neuronews:cost:daily_savings_potential
      expr: |
        (
          (neuronews:node_count:by_lifecycle{node_lifecycle="on-demand"} * 0.096 * 24) -
          (neuronews:node_count:by_lifecycle{node_lifecycle="spot"} * 0.029 * 24)
        ) * (neuronews:node_count:by_lifecycle{node_lifecycle="spot"} > 0)

    # Scale Events Tracking
    - record: neuronews:autoscaling:scale_up_events
      expr: increase(cluster_autoscaler_nodes_count[1h])

    - record: neuronews:autoscaling:scale_down_events
      expr: increase(cluster_autoscaler_scaled_down_nodes_total[1h])

    # DoD Success Metrics
    - alert: AutoscalingSuccessful
      expr: |
        (
          neuronews:autoscaling:scale_down_events > 0 and
          neuronews:resource_efficiency:cpu > 60 and
          neuronews:pod_distribution:by_node_type{node_lifecycle="spot"} > 0
        )
      for: 1m
      labels:
        severity: info
        component: success-tracking
      annotations:
        summary: "Autoscaling optimization successful"
        description: "Scale-down events occurring, resource efficiency > 60%, spot instances in use"

    - alert: BinPackingImproved
      expr: neuronews:binpacking:node_density > 5
      for: 5m
      labels:
        severity: info
        component: success-tracking
      annotations:
        summary: "Bin-packing efficiency improved"
        description: "Node {{ $labels.node }} has {{ $value }} pods, indicating good resource utilization"

  - name: neuronews.autoscaling.alerts
    interval: 30s
    rules:
    # Critical Alerts
    - alert: NoSpotInstancesAvailable
      expr: neuronews:node_count:by_lifecycle{node_lifecycle="spot"} == 0 and neuronews:pod_distribution:by_priority{priority_class="neuronews-batch"} > 0
      for: 5m
      labels:
        severity: warning
        component: spot-instances
      annotations:
        summary: "No spot instances available for batch workloads"
        description: "Batch workloads are running on on-demand instances due to spot unavailability"

    - alert: HighResourceWaste
      expr: neuronews:resource_efficiency:cpu < 30 or neuronews:resource_efficiency:memory < 40
      for: 10m
      labels:
        severity: warning
        component: resource-efficiency
      annotations:
        summary: "High resource waste detected"
        description: "CPU efficiency: {{ $value }}%, Memory efficiency may be low"

    # Success Notifications
    - alert: CostOptimizationActive
      expr: |
        (
          neuronews:cost:daily_savings_potential > 10 and
          neuronews:node_count:by_lifecycle{node_lifecycle="spot"} > 0
        )
      for: 1h
      labels:
        severity: info
        component: cost-tracking
      annotations:
        summary: "Cost optimization active - daily savings potential ${{ $value }}"
        description: "Spot instances in use, daily savings potential available"
